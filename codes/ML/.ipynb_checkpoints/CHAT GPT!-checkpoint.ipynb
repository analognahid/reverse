{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81f1f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nahid/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys,os, pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "\n",
    "\n",
    "with (open('DFGDATA.pkl' , \"rb\")) as openfile:\n",
    "    all_input_list , all_label_list = pickle.load(openfile)\n",
    "\n",
    "\n",
    "# input_text = [\"I love apples.\", \"She hates oranges.\", \"I enjoy reading books.\", \"He likes playing football.\"]\n",
    "\n",
    "# labels_task1 = [0, 1,0,1]  # Example task 1 labels\n",
    "# labels_task2 = [2, 3,4,5]  # Example task 2 labels\n",
    "\n",
    "input_text = all_input_list[:4]\n",
    "labels_task1 = all_label_list[:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0efd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE :  7718\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = BertTokenizer.from_pretrained(\"./binary-tokenizer\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "print('VOCAB SIZE : ', (tokenizer.vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466d32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# tokenizer.eos_token = \"<EOI>\"\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator(input_text_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fe9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Define the multitask BERT model\n",
    "\n",
    "\n",
    "class MultitaskBert(nn.Module):\n",
    "    def __init__(self, num_classes_task1, num_classes_task2):\n",
    "        super(MultitaskBert, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.task1_classifier = nn.Linear(768, num_classes_task1)\n",
    "        self.task2_classifier = nn.Linear(768, num_classes_task2)\n",
    "\n",
    "    def forward(self, t1_input_ids, t1_attention_mask , t2_input_ids, t2_attention_mask):\n",
    "        t1_outputs = self.bert(t1_input_ids, attention_mask=t1_attention_mask)\n",
    "        t1_pooled_output = t1_outputs.pooler_output\n",
    "        logits_task1 = self.task1_classifier(t1_pooled_output)\n",
    "        \n",
    "        \n",
    "        t2_outputs = self.bert(t2_input_ids, attention_mask=t2_attention_mask)\n",
    "        t2_pooled_output = t2_outputs.pooler_output\n",
    "        logits_task2 = self.task2_classifier(t2_pooled_output)\n",
    "\n",
    "        return logits_task1, logits_task2 , t1_outputs,t2_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc41b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO fix inputs = tokenizer(history, next_instruction, return_tensors='pt', \n",
    "#                    max_length=64, truncation=True, padding='max_length')\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt',)\n",
    "# encoded_inputs_task2 = tokenizer(input_text_task2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c6f7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False,  True, False,\n",
       "        False, False,  True,  True, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True,  True,  True, False,  True, False, False,  True, False,\n",
       "         True, False, False, False, False, False, False, False,  True,  True,\n",
       "        False, False, False, False,  True, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,  True,\n",
       "         True, False, False, False, False, False,  True,  True,  True, False,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True,  True, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLM\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs.labels[0]\n",
    "\n",
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "        (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "\n",
    "#TODO FIX 101, 102\n",
    "\n",
    "mask_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e27583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 18,\n",
       " 22,\n",
       " 23,\n",
       " 32,\n",
       " 36,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 65,\n",
       " 68,\n",
       " 70,\n",
       " 78,\n",
       " 79,\n",
       " 84,\n",
       " 86,\n",
       " 99,\n",
       " 100,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 113,\n",
       " 120,\n",
       " 133,\n",
       " 145,\n",
       " 146,\n",
       " 155,\n",
       " 163,\n",
       " 164]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "selection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f31e726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1908,   21,  180,    1,   28,   66,   68,    1,    1,    1,    1,\n",
       "          868,   57,   21,  233,  314,    9,  314,    1,   28,   66,   68,    1,\n",
       "            1,    1,    1,  868,   63,   21,   86,  416,    9,  101,    1,   28,\n",
       "           66,   68,    1,  868,   53,   21,  200,  123,    1,   28,   66,   68,\n",
       "            1,  868,   58,   21,   86,  101,    9,  122,    1,   28,   66,   68,\n",
       "            1,  868,   64,   21,  224,  122,    9, 2324,  226,    1,   28,   66,\n",
       "           68,    1,  127, 1284,   21,  162,   87,    1,   28,   66,   68,    1,\n",
       "          127, 1285,   21,  162,  122,    1,   28,   66,   68,    1,    1,    1,\n",
       "            1,  127,  421,   21,  233,  229,    9,  229,    1,   28,   66,   68,\n",
       "            1,    1,    1,    1,  127, 1286,   21,  233,  174,    9,  174,    1,\n",
       "           28,   66,   68,    1, 1911,   21,  116,  108,    9,   22,  107,    8,\n",
       "         1011,   23,    1,   28,   66,   68,    1,  127,  309,   21,  104,   95,\n",
       "           89,   22,  107,    8,  333,  397,   23,    1,   28,   66,   68,    1,\n",
       "          592,   62,   21,   31,   71,   67,    1,   28,   66,   68,    1,    3,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   2, 1910,   21,  116,  108,    9,   22,  107,    8,  333,  863,   23,\n",
       "            1,   28,   66,   68,    1, 1541,   55,   21,  116,   87,    9,   22,\n",
       "          107,    8,  333, 1291,   23,    1,   28,   66,   68,    1, 1541,   51,\n",
       "           21,  118,   87,    9,  108,    1,   28,   66,   68,    1, 1302,   50,\n",
       "           21,  125,  127,  288,    1,   28,   66,   68,    1, 1302,   65,   21,\n",
       "           86,   87,    9,   95,   89,   22,  107,    8,  657,  356,   23,    1,\n",
       "           28,   66,   68,    1, 1302,   58,   21,  133,   87,    9,   87,    1,\n",
       "           28,   66,   68,    1, 1302,   64,   21,  125,  127,  288,    1,   28,\n",
       "           66,   68,    1,    1,    1,    1, 1302,   54,   21,  119,   87,    1,\n",
       "           28,   66,   68,    1,    1,    1,    1,    1,    1,    1,  127,  288,\n",
       "           21,  183,    1,   28,   66,   68,    1,    1,    1,    1,    3,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   2,  456,   57,   21,   86,   87,    9,   95,   89,   22,  107,    8,\n",
       "          657,  988,   23,    1,   28,   66,   68,    1,    1,    1,    1,  456,\n",
       "           56,   21,  133,   87,    9,   87,    1,   28,   66,   68,    1,    1,\n",
       "            1,    1, 3494,   21,  125,  643,   52,    1,   28,   66,   68,    1,\n",
       "         2075,   21,  119,   87,    1,   28,   66,   68,    1,  643,   52,   21,\n",
       "          183,    1,   28,   66,   68,    1,  313,   59,   21,  116,  108,    9,\n",
       "           22,  107,    8, 1885,   53,   23,    1,   28,   66,   68,    1,  313,\n",
       "           55,   21,  116,  123,    9,   22,  107,    8, 1885,   61,   23,    1,\n",
       "           28,   66,   68,    1,    1,    1,    1,  313,   51,   21,  149,  123,\n",
       "            9,  108,    1,   28,   66,   68,    1,    1,    1,    1, 1030,   50,\n",
       "           21,   86,   87,    9,  123,    1,   28,   66,   68,    1, 1030,   57,\n",
       "           21,  448,  123,    9,  447,    1,   28,   66,   68,    1, 1030,   52,\n",
       "           21,  379,   87,    9,   14,    1,   28,   66,   68,    1, 1030,   60,\n",
       "           21,  115,  123,    9,   87,    1,   28,   66,   68,    1, 1030,   54,\n",
       "           21,  379,  123,    9,   12,    1,   28,   66,   68,    1,  456,   61,\n",
       "           21,  125,  643,   52,    1,   28,   66,   68,    1,    3,    0],\n",
       "        [   2,  601,   64,   21,  162,   94,    1,   28,   66,   68,    1,    1,\n",
       "            1,    1,  601,   51,   21,  118,   95,   89,   22,  107,    8,  657,\n",
       "          526,   23,    9,   11,    1,   28,   66,   68,    1,    1,    1,    1,\n",
       "          619,   21,   86,   94,    9,  122,    1,   28,   66,   68,    1,  737,\n",
       "           53,   21,  125,  176,  277,    1,   28,   66,   68,    1,  176,  277,\n",
       "           21,  104, 1910,    1,   28,   66,   68,    1,  176,  445,   21,   86,\n",
       "          129,   89,   22,  107,    8,  657,  317,   23,    9,   12,    1,   28,\n",
       "           66,   68,    1,  876,   65,   21,  200,   94,    1,   28,   66,   68,\n",
       "            1,  876,   57,   21,  183,    1,   28,   66,   68,    1,  737,   56,\n",
       "           21,   86,  108,    9,   95,   89,   22,  107,    8,  657,  515,   23,\n",
       "            1,   28,   66,   68,    1,  176, 1123,   21,  104,  636,   59,    1,\n",
       "           28,   66,   68,    1,  876,   52,   21,  183,    1,   28,   66,   68,\n",
       "            1,  601,   59,   21,  180,    1,   28,   66,   68,    1,    1,    1,\n",
       "            1,  601,   57,   21,  118,  129,   89,   22,  107,    8,  657,  988,\n",
       "           23,    9,   11,    1,   28,   66,   68,    1,    1,    1,    1,  601,\n",
       "           56,   21,  139,  876,   52,    1,   28,   66,   68,    1,    3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628bfa85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5d4089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  103,   21,  180,    1,   28,   66,   68,    1,    1,    1,    1,\n",
       "          868,   57,  103,  233,  314,    9,  103,    1,   28,   66,  103,  103,\n",
       "            1,    1,    1,  868,   63,   21,   86,  416,  103,  101,    1,   28,\n",
       "          103,   68,    1,  868,   53,   21,  200,  123,    1,   28,   66,   68,\n",
       "            1,  868,   58,   21,   86,  101,    9,  122,    1,   28,   66,   68,\n",
       "            1,  103,  103,  103,  224,  103,    9, 2324,  103,    1,  103,   66,\n",
       "           68,    1,  127, 1284,   21,  162,  103,  103,   28,   66,   68,    1,\n",
       "          103, 1285,  103,  162,  122,    1,   28,   66,   68,    1,    1,    1,\n",
       "            1,  127,  421,  103,  103,  229,    9,  229,    1,   28,  103,  103,\n",
       "          103,    1,    1,    1,  127,  103,   21,  233,  174,    9,  174,    1,\n",
       "          103,   66,   68,    1, 1911,   21,  116,  108,    9,   22,  107,    8,\n",
       "         1011,  103,    1,   28,   66,   68,    1,  127,  309,   21,  104,   95,\n",
       "           89,  103,  103,    8,  333,  397,   23,    1,   28,   66,   68,  103,\n",
       "          592,   62,   21,   31,   71,   67,    1,  103,  103,   68,    1,    3,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   2, 1910,   21,  116,  108,  103,   22,  107,    8,  333,  863,   23,\n",
       "            1,   28,   66,   68,    1, 1541,   55,  103,  116,   87,    9,   22,\n",
       "          107,    8,  103, 1291,   23,    1,  103,   66,   68,    1, 1541,   51,\n",
       "           21,  118,   87,    9,  108,    1,   28,   66,   68,    1, 1302,  103,\n",
       "          103,  125,  103,  103,    1,   28,   66,   68,    1, 1302,  103,   21,\n",
       "           86,   87,    9,   95,   89,   22,  107,    8,  657,  356,   23,    1,\n",
       "           28,   66,   68,    1, 1302,   58,   21,  133,   87,    9,  103,    1,\n",
       "           28,   66,   68,    1, 1302,   64,   21,  125,  127,  288,  103,   28,\n",
       "           66,   68,    1,    1,    1,    1,  103,   54,   21,  119,  103,    1,\n",
       "           28,   66,   68,    1,    1,    1,    1,  103,    1,    1,  103,  288,\n",
       "          103,  183,    1,   28,   66,   68,    1,    1,    1,    1,    3,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 103,  456,   57,   21,   86,   87,  103,   95,   89,   22,  107,  103,\n",
       "          103,  988,  103,    1,   28,   66,   68,    1,  103,    1,  103,  456,\n",
       "           56,   21,  133,   87,    9,  103,    1,   28,  103,   68,    1,  103,\n",
       "            1,    1, 3494,   21,  125,  643,   52,    1,  103,   66,   68,  103,\n",
       "          103,   21,  119,   87,    1,   28,   66,  103,  103,  103,   52,   21,\n",
       "          183,    1,   28,   66,   68,    1,  313,   59,   21,  103,  108,  103,\n",
       "           22,  107,    8, 1885,  103,   23,    1,   28,   66,   68,    1,  103,\n",
       "           55,   21,  116,  123,    9,   22,  107,    8, 1885,   61,   23,    1,\n",
       "           28,   66,   68,    1,    1,  103,  103,  313,   51,   21,  149,  123,\n",
       "            9,  103,    1,   28,   66,   68,  103,  103,    1,    1, 1030,   50,\n",
       "           21,   86,  103,    9,  123,    1,   28,   66,   68,    1, 1030,   57,\n",
       "          103,  448,  123,    9,  447,    1,   28,   66,   68,    1, 1030,   52,\n",
       "           21,  103,   87,  103,   14,  103,   28,   66,   68,    1,  103,   60,\n",
       "          103,  115,  123,    9,   87,    1,   28,   66,   68,  103, 1030,   54,\n",
       "           21,  379,  123,    9,   12,    1,   28,   66,   68,    1,  456,   61,\n",
       "           21,  125,  643,   52,    1,   28,   66,   68,    1,    3,    0],\n",
       "        [   2,  601,   64,   21,  103,   94,  103,  103,   66,   68,    1,    1,\n",
       "            1,    1,  601,   51,   21,  118,  103,   89,  103,  107,    8,  103,\n",
       "          526,   23,  103,   11,    1,   28,  103,  103,    1,    1,    1,    1,\n",
       "          619,  103,   86,   94,    9,  122,    1,   28,   66,   68,    1,  737,\n",
       "           53,   21,  125,  176,  103,    1,   28,   66,   68,    1,  103,  277,\n",
       "           21,  104, 1910,    1,  103,   66,  103,  103,  176,  445,   21,   86,\n",
       "          129,   89,   22,  107,    8,  103,  317,   23,    9,   12,    1,   28,\n",
       "           66,   68,  103,  876,   65,   21,  103,   94,    1,   28,   66,   68,\n",
       "            1,  876,  103,   21,  183,    1,   28,   66,   68,    1,  737,   56,\n",
       "           21,   86,  103,  103,   95,  103,   22,  107,    8,  657,  103,   23,\n",
       "            1,   28,   66,   68,    1,  176, 1123,  103,  104,  103,   59,    1,\n",
       "          103,   66,   68,    1,  876,   52,   21,  183,    1,   28,   66,   68,\n",
       "            1,  601,   59,   21,  180,    1,   28,   66,   68,    1,    1,    1,\n",
       "            1,  601,  103,  103,  103,  129,   89,   22,  107,  103,  657,  988,\n",
       "          103,    9,   11,    1,   28,   66,  103,    1,    1,    1,    1,  103,\n",
       "           56,   21,  139,  103,   52,    1,   28,   66,   68,    1,    3]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065a506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2faad2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "print(inputs.token_type_ids)\n",
    "inputs.token_type_ids[0][0] =1\n",
    "print(inputs.token_type_ids)\n",
    "#TODO set token type ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b37a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "model = MultitaskBert(num_classes_task1=2, num_classes_task2=VOCAB_SIZE)  # Example number of classes for each task\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW( model.parameters() , lr=5e-6)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "910c6519",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 1908,   21,  180,    1,   28,   66,   68,    1,    1,    1,    1,\n",
      "          868,   57,   21,  233,  314,    9,  314,    1,   28,   66,   68,    1,\n",
      "            1,    1,    1,  868,   63,   21,   86,  416,    9,  101,    1,   28,\n",
      "           66,   68,    1,  868,   53,   21,  200,  123,    1,   28,   66,   68,\n",
      "            1,  868,   58,   21,   86,  101,    9,  122,    1,   28,   66,   68,\n",
      "            1,  868,   64,   21,  224,  122,    9, 2324,  226,    1,   28,   66,\n",
      "           68,    1,  127, 1284,   21,  162,   87,    1,   28,   66,   68,    1,\n",
      "          127, 1285,   21,  162,  122,    1,   28,   66,   68,    1,    1,    1,\n",
      "            1,  127,  421,   21,  233,  229,    9,  229,    1,   28,   66,   68,\n",
      "            1,    1,    1,    1,  127, 1286,   21,  233,  174,    9,  174,    1,\n",
      "           28,   66,   68,    1, 1911,   21,  116,  108,    9,   22,  107,    8,\n",
      "         1011,   23,    1,   28,   66,   68,    1,  127,  309,   21,  104,   95,\n",
      "           89,   22,  107,    8,  333,  397,   23,    1,   28,   66,   68,    1,\n",
      "          592,   62,   21,   31,   71,   67,    1,   28,   66,   68,    1,    3,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   2, 1910,   21,  116,  108,    9,   22,  107,    8,  333,  863,   23,\n",
      "            1,   28,   66,   68,    1, 1541,   55,   21,  116,   87,    9,   22,\n",
      "          107,    8,  333, 1291,   23,    1,   28,   66,   68,    1, 1541,   51,\n",
      "           21,  118,   87,    9,  108,    1,   28,   66,   68,    1, 1302,   50,\n",
      "           21,  125,  127,  288,    1,   28,   66,   68,    1, 1302,   65,   21,\n",
      "           86,   87,    9,   95,   89,   22,  107,    8,  657,  356,   23,    1,\n",
      "           28,   66,   68,    1, 1302,   58,   21,  133,   87,    9,   87,    1,\n",
      "           28,   66,   68,    1, 1302,   64,   21,  125,  127,  288,    1,   28,\n",
      "           66,   68,    1,    1,    1,    1, 1302,   54,   21,  119,   87,    1,\n",
      "           28,   66,   68,    1,    1,    1,    1,    1,    1,    1,  127,  288,\n",
      "           21,  183,    1,   28,   66,   68,    1,    1,    1,    1,    3,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   2,  456,   57,   21,   86,   87,    9,   95,   89,   22,  107,    8,\n",
      "          657,  988,   23,    1,   28,   66,   68,    1,    1,    1,    1,  456,\n",
      "           56,   21,  133,   87,    9,   87,    1,   28,   66,   68,    1,    1,\n",
      "            1,    1, 3494,   21,  125,  643,   52,    1,   28,   66,   68,    1,\n",
      "         2075,   21,  119,   87,    1,   28,   66,   68,    1,  643,   52,   21,\n",
      "          183,    1,   28,   66,   68,    1,  313,   59,   21,  116,  108,    9,\n",
      "           22,  107,    8, 1885,   53,   23,    1,   28,   66,   68,    1,  313,\n",
      "           55,   21,  116,  123,    9,   22,  107,    8, 1885,   61,   23,    1,\n",
      "           28,   66,   68,    1,    1,    1,    1,  313,   51,   21,  149,  123,\n",
      "            9,  108,    1,   28,   66,   68,    1,    1,    1,    1, 1030,   50,\n",
      "           21,   86,   87,    9,  123,    1,   28,   66,   68,    1, 1030,   57,\n",
      "           21,  448,  123,    9,  447,    1,   28,   66,   68,    1, 1030,   52,\n",
      "           21,  379,   87,    9,   14,    1,   28,   66,   68,    1, 1030,   60,\n",
      "           21,  115,  123,    9,   87,    1,   28,   66,   68,    1, 1030,   54,\n",
      "           21,  379,  123,    9,   12,    1,   28,   66,   68,    1,  456,   61,\n",
      "           21,  125,  643,   52,    1,   28,   66,   68,    1,    3,    0],\n",
      "        [   2,  601,   64,   21,  162,   94,    1,   28,   66,   68,    1,    1,\n",
      "            1,    1,  601,   51,   21,  118,   95,   89,   22,  107,    8,  657,\n",
      "          526,   23,    9,   11,    1,   28,   66,   68,    1,    1,    1,    1,\n",
      "          619,   21,   86,   94,    9,  122,    1,   28,   66,   68,    1,  737,\n",
      "           53,   21,  125,  176,  277,    1,   28,   66,   68,    1,  176,  277,\n",
      "           21,  104, 1910,    1,   28,   66,   68,    1,  176,  445,   21,   86,\n",
      "          129,   89,   22,  107,    8,  657,  317,   23,    9,   12,    1,   28,\n",
      "           66,   68,    1,  876,   65,   21,  200,   94,    1,   28,   66,   68,\n",
      "            1,  876,   57,   21,  183,    1,   28,   66,   68,    1,  737,   56,\n",
      "           21,   86,  108,    9,   95,   89,   22,  107,    8,  657,  515,   23,\n",
      "            1,   28,   66,   68,    1,  176, 1123,   21,  104,  636,   59,    1,\n",
      "           28,   66,   68,    1,  876,   52,   21,  183,    1,   28,   66,   68,\n",
      "            1,  601,   59,   21,  180,    1,   28,   66,   68,    1,    1,    1,\n",
      "            1,  601,   57,   21,  118,  129,   89,   22,  107,    8,  657,  988,\n",
      "           23,    9,   11,    1,   28,   66,   68,    1,    1,    1,    1,  601,\n",
      "           56,   21,  139,  876,   52,    1,   28,   66,   68,    1,    3]])\n",
      "tensor([[   2, 1908,   21,  180,    1,   28,   66,   68,    1,    1,    1,    1,\n",
      "          868,   57,   21,  233,  314,    9,  314,    1,   28,   66,   68,    1,\n",
      "            1,    1,    1,  868,   63,   21,   86,  416,    9,  101,    1,   28,\n",
      "           66,   68,    1,  868,   53,   21,  200,  123,    1,   28,   66,   68,\n",
      "            1,  868,   58,   21,   86,  101,    9,  122,    1,   28,   66,   68,\n",
      "            1,  868,   64,   21,  224,  122,    9, 2324,  226,    1,   28,   66,\n",
      "           68,    1,  127, 1284,   21,  162,   87,    1,   28,   66,   68,    1,\n",
      "          127, 1285,   21,  162,  122,    1,   28,   66,   68,    1,    1,    1,\n",
      "            1,  127,  421,   21,  233,  229,    9,  229,    1,   28,   66,   68,\n",
      "            1,    1,    1,    1,  127, 1286,   21,  233,  174,    9,  174,    1,\n",
      "           28,   66,   68,    1, 1911,   21,  116,  108,    9,   22,  107,    8,\n",
      "         1011,   23,    1,   28,   66,   68,    1,  127,  309,   21,  104,   95,\n",
      "           89,   22,  107,    8,  333,  397,   23,    1,   28,   66,   68,    1,\n",
      "          592,   62,   21,   31,   71,   67,    1,   28,   66,   68,    1,    3,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   2, 1910,   21,  116,  108,    9,   22,  107,    8,  333,  863,   23,\n",
      "            1,   28,   66,   68,    1, 1541,   55,   21,  116,   87,    9,   22,\n",
      "          107,    8,  333, 1291,   23,    1,   28,   66,   68,    1, 1541,   51,\n",
      "           21,  118,   87,    9,  108,    1,   28,   66,   68,    1, 1302,   50,\n",
      "           21,  125,  127,  288,    1,   28,   66,   68,    1, 1302,   65,   21,\n",
      "           86,   87,    9,   95,   89,   22,  107,    8,  657,  356,   23,    1,\n",
      "           28,   66,   68,    1, 1302,   58,   21,  133,   87,    9,   87,    1,\n",
      "           28,   66,   68,    1, 1302,   64,   21,  125,  127,  288,    1,   28,\n",
      "           66,   68,    1,    1,    1,    1, 1302,   54,   21,  119,   87,    1,\n",
      "           28,   66,   68,    1,    1,    1,    1,    1,    1,    1,  127,  288,\n",
      "           21,  183,    1,   28,   66,   68,    1,    1,    1,    1,    3,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   2,  456,   57,   21,   86,   87,    9,   95,   89,   22,  107,    8,\n",
      "          657,  988,   23,    1,   28,   66,   68,    1,    1,    1,    1,  456,\n",
      "           56,   21,  133,   87,    9,   87,    1,   28,   66,   68,    1,    1,\n",
      "            1,    1, 3494,   21,  125,  643,   52,    1,   28,   66,   68,    1,\n",
      "         2075,   21,  119,   87,    1,   28,   66,   68,    1,  643,   52,   21,\n",
      "          183,    1,   28,   66,   68,    1,  313,   59,   21,  116,  108,    9,\n",
      "           22,  107,    8, 1885,   53,   23,    1,   28,   66,   68,    1,  313,\n",
      "           55,   21,  116,  123,    9,   22,  107,    8, 1885,   61,   23,    1,\n",
      "           28,   66,   68,    1,    1,    1,    1,  313,   51,   21,  149,  123,\n",
      "            9,  108,    1,   28,   66,   68,    1,    1,    1,    1, 1030,   50,\n",
      "           21,   86,   87,    9,  123,    1,   28,   66,   68,    1, 1030,   57,\n",
      "           21,  448,  123,    9,  447,    1,   28,   66,   68,    1, 1030,   52,\n",
      "           21,  379,   87,    9,   14,    1,   28,   66,   68,    1, 1030,   60,\n",
      "           21,  115,  123,    9,   87,    1,   28,   66,   68,    1, 1030,   54,\n",
      "           21,  379,  123,    9,   12,    1,   28,   66,   68,    1,  456,   61,\n",
      "           21,  125,  643,   52,    1,   28,   66,   68,    1,    3,    0],\n",
      "        [   2,  601,   64,   21,  162,   94,    1,   28,   66,   68,    1,    1,\n",
      "            1,    1,  601,   51,   21,  118,   95,   89,   22,  107,    8,  657,\n",
      "          526,   23,    9,   11,    1,   28,   66,   68,    1,    1,    1,    1,\n",
      "          619,   21,   86,   94,    9,  122,    1,   28,   66,   68,    1,  737,\n",
      "           53,   21,  125,  176,  277,    1,   28,   66,   68,    1,  176,  277,\n",
      "           21,  104, 1910,    1,   28,   66,   68,    1,  176,  445,   21,   86,\n",
      "          129,   89,   22,  107,    8,  657,  317,   23,    9,   12,    1,   28,\n",
      "           66,   68,    1,  876,   65,   21,  200,   94,    1,   28,   66,   68,\n",
      "            1,  876,   57,   21,  183,    1,   28,   66,   68,    1,  737,   56,\n",
      "           21,   86,  108,    9,   95,   89,   22,  107,    8,  657,  515,   23,\n",
      "            1,   28,   66,   68,    1,  176, 1123,   21,  104,  636,   59,    1,\n",
      "           28,   66,   68,    1,  876,   52,   21,  183,    1,   28,   66,   68,\n",
      "            1,  601,   59,   21,  180,    1,   28,   66,   68,    1,    1,    1,\n",
      "            1,  601,   57,   21,  118,  129,   89,   22,  107,    8,  657,  988,\n",
      "           23,    9,   11,    1,   28,   66,   68,    1,    1,    1,    1,  601,\n",
      "           56,   21,  139,  876,   52,    1,   28,   66,   68,    1,    3]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(inputs['labels'])\n",
    "print(torch.squeeze(inputs['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422f5e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m labels_task1_tensor \u001b[38;5;241m=\u001b[39m labels_task1_tensor\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor)\n\u001b[1;32m     18\u001b[0m loss_task1 \u001b[38;5;241m=\u001b[39m criterion(logits_task1, labels_task1_tensor)\n\u001b[0;32m---> 20\u001b[0m loss_task2 \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_task2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_task1, loss_task2)\n\u001b[1;32m     24\u001b[0m (loss_task1\u001b[38;5;241m+\u001b[39mloss_task2)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# task2-mlm\n",
    "# task 1 - dfd\n",
    "\n",
    "# Forward pass\n",
    "optim.zero_grad()\n",
    "logits_task1, logits_task2 , t1_outputs,t2_outputs = model(inputs['input_ids'], inputs['attention_mask'], inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "predictions_task1 = torch.argmax(logits_task1, dim=1)\n",
    "predictions_task2 = torch.argmax(logits_task2, dim=1)\n",
    "\n",
    "# labels_ = batch['labels'].to(device)\n",
    "# labels_task1_tensor = torch.Tensor(labels_task1)\n",
    "# labels_task1_tensor = labels_task1_tensor.type(torch.LongTensor)\n",
    "\n",
    "\n",
    "labels_task1_tensor = torch.Tensor(labels_task1)\n",
    "labels_task1_tensor = labels_task1_tensor.type(torch.LongTensor)\n",
    "loss_task1 = criterion(logits_task1, labels_task1_tensor)\n",
    "\n",
    "loss_task2 = criterion(logits_task2, inputs['labels'])\n",
    "\n",
    "print(loss_task1, loss_task2)\n",
    "\n",
    "(loss_task1+loss_task2).backward()\n",
    "# update parameters\n",
    "optim.step()\n",
    "        \n",
    "print(\"Task 1 predictions:\", predictions_task1)\n",
    "print(\"Task 2 predictions:\", predictions_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda20e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_task1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b155f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94abbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1bcce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
