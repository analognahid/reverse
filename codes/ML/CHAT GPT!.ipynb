{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedbfe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nahid/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys,os, pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "\n",
    "\n",
    "with (open('DFGDATA.pkl' , \"rb\")) as openfile:\n",
    "    all_input_list , all_label_list = pickle.load(openfile)\n",
    "\n",
    "\n",
    "# input_text = [\"I love apples.\", \"She hates oranges.\", \"I enjoy reading books.\", \"He likes playing football.\"]\n",
    "\n",
    "# labels_task1 = [0, 1,0,1]  # Example task 1 labels\n",
    "# labels_task2 = [2, 3,4,5]  # Example task 2 labels\n",
    "\n",
    "input_text = all_input_list[:8]\n",
    "labels_dfg = all_label_list[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0785c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE :  7718\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = BertTokenizer.from_pretrained(\"./binary-tokenizer\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "print('VOCAB SIZE : ', (tokenizer.vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2010bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# tokenizer.eos_token = \"<EOI>\"\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452ac11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator(input_text_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e098e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO fix inputs = tokenizer(history, next_instruction, return_tensors='pt', \n",
    "#                    max_length=64, truncation=True, padding='max_length')\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt',)\n",
    "# encoded_inputs_task2 = tokenizer(input_text_task2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c51866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLM\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs.labels[0]\n",
    "\n",
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "        (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "\n",
    "#TODO FIX 101, 102\n",
    "\n",
    "# mask_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3595df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "# selection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e9ba8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb005cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02cdfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c27903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8040e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "print(inputs.token_type_ids)\n",
    "inputs.token_type_ids[0][0] =1\n",
    "print(inputs.token_type_ids)\n",
    "#TODO set token type ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed2fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7718\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ad13b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Define the multitask BERT model\n",
    "\n",
    "hidden_size = BertModel.from_pretrained('bert-base-uncased').config.hidden_size\n",
    "# vocab_size = bert_model.config.vocab_size\n",
    "class MultitaskBert(nn.Module):\n",
    "    def __init__(self, num_classes_task1, num_classes_task2):\n",
    "        super(MultitaskBert, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased') #BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(  hidden_size , VOCAB_SIZE)\n",
    "        self.task2_classifier = nn.Linear(hidden_size, 2) #todo was 768 num_classes_task2\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, t1_input_ids, t1_attention_mask , t2_input_ids, t2_attention_mask):\n",
    "        bert_outputs = self.bert(t1_input_ids, attention_mask=t1_attention_mask)\n",
    "        \n",
    "#         logits_mlm = bert_outputs.last_hidden_state\n",
    "        hidden_states = bert_outputs.last_hidden_state\n",
    "        logits_mlm = self.fc(hidden_states)\n",
    "        \n",
    "        bert_pooled_output = bert_outputs.pooler_output\n",
    "        logits_task2 = self.task2_classifier(bert_pooled_output)\n",
    "        \n",
    "        \n",
    "#         t2_outputs = self.bert(t2_input_ids, attention_mask=t2_attention_mask)\n",
    "#         t2_pooled_output = t2_outputs.pooler_output\n",
    "#         t2_outputs = self.task2_classifier(t1_input_ids, attention_mask=t1_attention_mask)#TODO t1/t2 fix\n",
    "\n",
    "        return logits_mlm,   logits_task2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd1c92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "model = MultitaskBert(num_classes_task1=VOCAB_SIZE, num_classes_task2=2)  # Example number of classes for each task\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW( model.parameters() , lr=5e-6)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44e711c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(inputs['labels'])\n",
    "# print(torch.squeeze(inputs['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513ac27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 7718]) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "optim.zero_grad()\n",
    "# logits_task1, logits_task2 , t1_outputs,t2_outputs = model(inputs['input_ids'], inputs['attention_mask'], inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "logits_task1,   logits_task2 = model(inputs['input_ids'], inputs['attention_mask'], inputs['input_ids'], inputs['attention_mask'])\n",
    "print(logits_task1.shape, logits_task2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ffc5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 7718]) torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "print(logits_task1.shape , inputs['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fe4cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 7718]), torch.Size([4096]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_task1.view(-1, VOCAB_SIZE).shape ,inputs['labels'].view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6045c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_task1 = criterion(logits_task1.view(-1, VOCAB_SIZE), inputs['labels'].view(-1))\n",
    "predictions_task1 = torch.argmax(logits_task1, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f410fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dfg =  torch.Tensor(labels_dfg)\n",
    "labels_dfg = labels_dfg.type(torch.LongTensor)\n",
    "labels_dfg.shape\n",
    "logits_task2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a997fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_task2 = criterion(logits_task2, labels_dfg)\n",
    "predictions_task2 = torch.argmax(logits_task2, dim=1)\n",
    "predictions_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ecf940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss_task1+loss_task2).backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dc7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
