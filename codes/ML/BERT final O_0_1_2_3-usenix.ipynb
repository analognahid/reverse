{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af388b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device ,torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c86ae04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de072e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os, pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score,f1_score\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME=\"TYPE_WRITER_BERT_final_O_0123\"\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "MAX_TOKEN_SIZE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d98a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\n",
    "             '/home/raisul/type_data/instructions_and_type_data_100k/',\n",
    "              '/home/raisul/type_data/O1_mix/instructions_and_type_data_100k',\n",
    "              '/media/raisul/nahid_personal/optimizations/O2_ghidra/instructions_and_type_data_100k_mix/',\n",
    "              '/home/raisul/type_data/O3_mix/instructions_and_type_data_100k'\n",
    "             ]\n",
    "\n",
    "all_bins_paths = []\n",
    "for oi,data_path in enumerate(data_paths):\n",
    "    for fi,pkl_file_name in enumerate(os.listdir(data_path)):\n",
    "            if fi>200000:\n",
    "                 break\n",
    "            user_of_proj = pkl_file_name.split('_____')[0]\n",
    "            pkl_path = os.path.join(data_path,pkl_file_name)\n",
    "            all_bins_paths.append( (pkl_path,oi ,user_of_proj) )\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d5d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_types(type_name):\n",
    "    global TYPE_FIX_MAP\n",
    "    \n",
    "    prev_type_name= type_name\n",
    "    \n",
    "    if '*' in type_name:\n",
    "        if type_name.rindex('*')>0:#reduce to single * (rightmost, ignore array before that)\n",
    "            type_name = type_name[ type_name.rindex('*'): ]\n",
    "    if 'array_' in type_name:\n",
    "        if type_name.rindex('array_')>0:\n",
    "            type_name = type_name[ type_name.rindex('array_'): ]\n",
    "    if 'unsigned char' in type_name:\n",
    "        type_name = type_name.replace('unsigned char' , 'char')\n",
    "        \n",
    "        \n",
    "    if type_name not in TYPE_FIX_MAP:\n",
    "        TYPE_FIX_MAP[prev_type_name] = type_name\n",
    "    return type_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5c4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_SAVE_PATH =  '/home/raisul/models/'\n",
    "TYPE_COUNT ={}\n",
    "FINAL_TYPE_COUNT = {}\n",
    "TYPE_PROBABILITY = {}\n",
    "\n",
    "\n",
    "REJECT = ['union' , 'enumeration' , 'int128' , '_Bool' , 'complex' ,'bool']\n",
    "TYPE_FIX_MAP = {}\n",
    "MAX_TYPE_SAMPLE  =20*1000\n",
    "TYPE_MAPPING = {}\n",
    "\n",
    "\n",
    "ALL_INPUT_LIST = []\n",
    "ALL_INPUT_SLICE_INFO  = []\n",
    "ALL_LABEL_LIST = []\n",
    "ALL_OPT_LIST = []\n",
    "\n",
    "TRAIN_SAMPLE_COUNT = 0\n",
    "VAL_SAMPLE_COUNT   =0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37270a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'int': 7780112, '*char': 621750, 'structure': 1236172, 'long int': 110663, '*int': 818128, 'array_int': 1598244, 'char': 253619, 'array_char': 442244, '*structure': 1165780, 'short int': 23431, 'array_structure': 15820, 'float': 273813, 'double': 171007, 'unsigned int': 157359, '*double': 18913, 'long unsigned int': 32338, '*float': 14241, 'enumeration': 10864, 'union': 4170, 'array_float': 65766, 'long long int': 62716, 'array_double': 47898, 'array_short int': 5118, '_Bool': 1003, 'short unsigned int': 14611, '*union': 8636, '*unsigned int': 10855, 'long long unsigned int': 29475, '*long int': 12763, '*enumeration': 977, 'signed char': 134224, '*long long unsigned int': 931, 'array_unsigned int': 38572, 'array_long long int': 10356, '*long unsigned int': 3910, 'array_long long unsigned int': 944, 'array_long int': 12747, 'array_long unsigned int': 866, '*short int': 4416, '*_Bool': 145, '*long long int': 5388, '*short unsigned int': 2232, 'array_short unsigned int': 833, '*signed char': 171282, 'array_signed char': 1478704, 'array_enumeration': 12989, 'complex double': 17, 'array_union': 81, 'bool': 10, '__int128': 16, '__int128 unsigned': 254, '*__int128 unsigned': 10, 'array___int128 unsigned': 11, 'array__Bool': 12, '*long double': 4, 'long double': 1092, 'unsigned long int': 126045, 'unsigned long long int': 2375, '*void': 133140, 'array_unsigned long long int': 8253, 'unsigned short int': 6952, 'array_unsigned short int': 2279, '*unsigned long int': 1346, 'array_unsigned long int': 1668, '*unsigned short int': 443, '*unsigned long long int': 197, 'array_long double': 109}\n",
      "DBG 1 union union\n",
      "DBG 1 union *union\n",
      "DBG 1 union array_union\n",
      "DBG 2 union union\n",
      "DBG 1 enumeration enumeration\n",
      "DBG 1 enumeration *enumeration\n",
      "DBG 1 enumeration array_enumeration\n",
      "DBG 2 enumeration enumeration\n",
      "DBG 1 int128 __int128\n",
      "DBG 1 int128 __int128 unsigned\n",
      "DBG 1 int128 *__int128 unsigned\n",
      "DBG 1 int128 array___int128 unsigned\n",
      "DBG 2 int int128\n",
      "DBG 1 _Bool _Bool\n",
      "DBG 1 _Bool *_Bool\n",
      "DBG 1 _Bool array__Bool\n",
      "DBG 2 _Bool _Bool\n",
      "DBG 1 complex complex double\n",
      "DBG 1 bool bool\n",
      "DBG 2 bool bool\n",
      "temp_removed_keys ['union', '*union', 'array_union', 'union', 'enumeration', '*enumeration', 'array_enumeration', 'enumeration', '__int128', '__int128 unsigned', '*__int128 unsigned', 'array___int128 unsigned', 'int128', '_Bool', '*_Bool', 'array__Bool', '_Bool', 'complex double', 'bool', 'bool']\n",
      "20000 7780112 int\n",
      "20000 1598244 array_int\n",
      "20000 1478704 array_signed char\n",
      "20000 1236172 structure\n",
      "20000 1165780 *structure\n",
      "20000 818128 *int\n",
      "20000 621750 *char\n",
      "20000 442244 array_char\n",
      "20000 273813 float\n",
      "20000 253619 char\n",
      "20000 171282 *signed char\n",
      "20000 171007 double\n",
      "20000 157359 unsigned int\n",
      "20000 134224 signed char\n",
      "20000 133140 *void\n",
      "20000 126045 unsigned long int\n",
      "20000 110663 long int\n",
      "20000 65766 array_float\n",
      "20000 62716 long long int\n",
      "20000 47898 array_double\n",
      "20000 38572 array_unsigned int\n",
      "20000 32338 long unsigned int\n",
      "20000 29475 long long unsigned int\n",
      "20000 23431 short int\n",
      "20000 18913 *double\n",
      "20000 15820 array_structure\n",
      "20000 14611 short unsigned int\n",
      "20000 14241 *float\n",
      "20000 12763 *long int\n",
      "20000 12747 array_long int\n",
      "20000 10855 *unsigned int\n",
      "20000 10356 array_long long int\n",
      "20000 8253 array_unsigned long long int\n",
      "20000 6952 unsigned short int\n",
      "20000 5388 *long long int\n",
      "20000 5118 array_short int\n",
      "20000 4416 *short int\n",
      "20000 3910 *long unsigned int\n",
      "20000 2375 unsigned long long int\n",
      "20000 2279 array_unsigned short int\n",
      "20000 2232 *short unsigned int\n",
      "20000 1668 array_unsigned long int\n",
      "20000 1346 *unsigned long int\n",
      "20000 1092 long double\n",
      "20000 944 array_long long unsigned int\n",
      "20000 931 *long long unsigned int\n",
      "20000 866 array_long unsigned int\n",
      "20000 833 array_short unsigned int\n",
      "20000 443 *unsigned short int\n",
      "20000 197 *unsigned long long int\n",
      "20000 109 array_long double\n",
      "20000 4 *long double\n",
      "{'int': 0.002570657080515036, 'array_int': 0.0125137338228706, 'array_signed char': 0.013525357339940921, 'structure': 0.016178978329876425, '*structure': 0.017155895623531026, '*int': 0.02444605245145014, '*char': 0.03216726980297547, 'array_char': 0.04522390354645851, 'float': 0.07304255093804896, 'char': 0.07885844514803701, '*signed char': 0.1167665020258988, 'double': 0.11695427672551416, 'unsigned int': 0.1270979098748721, 'signed char': 0.14900464894504709, '*void': 0.1502178158329578, 'unsigned long int': 0.15867348962672062, 'long int': 0.1807288795713111, 'array_float': 0.30410850591491045, 'long long int': 0.31889788889597553, 'array_double': 0.4175539688504739, 'array_unsigned int': 0.5185108368764907, 'long unsigned int': 0.6184674376894057, 'long long unsigned int': 0.6785411365564037, 'short int': 0.8535700567624088, '*double': 1.0574736953418284, 'array_structure': 1.2642225031605563, 'short unsigned int': 1.3688317021422216, '*float': 1.4043957587248086, '*long int': 1.5670296952127243, 'array_long int': 1.5689966266572526, '*unsigned int': 1.842468908337172, 'array_long long int': 1.9312475859405176, 'array_unsigned long long int': 2.423361201987156, 'unsigned short int': 2.8768699654775602, '*long long int': 3.7119524870081664, 'array_short int': 3.9077764751856194, '*short int': 4.528985507246377, '*long unsigned int': 5.115089514066496, 'unsigned long long int': 8.421052631578947, 'array_unsigned short int': 8.775778850372971, '*short unsigned int': 8.960573476702509, 'array_unsigned long int': 11.990407673860911, '*unsigned long int': 14.85884101040119, 'long double': 18.315018315018314, 'array_long long unsigned int': 21.1864406779661, '*long long unsigned int': 21.482277121374867, 'array_long unsigned int': 23.094688221709006, 'array_short unsigned int': 24.009603841536613, '*unsigned short int': 45.146726862302486, '*unsigned long long int': 101.5228426395939, 'array_long double': 183.4862385321101, '*long double': 5000.0}\n",
      "118203 502399\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "#TODO dont load all at a time in memry\n",
    "def make_dataset():\n",
    "    global TYPE_FIX_MAP, REJECT ,TYPE_COUNT,TYPE_MAPPING,TRAIN_SAMPLE_COUNT,VAL_SAMPLE_COUNT\n",
    "    \n",
    "    dbg_temp = []\n",
    "    \n",
    "    #count how many real samples\n",
    "    for pkl_path, oi, user_proj_name in all_bins_paths:\n",
    "\n",
    "\n",
    "        with open(pkl_path, 'rb') as file:\n",
    "            try:\n",
    "                model_input_list, model_type_list = pickle.load(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            for i in range(len(model_input_list)):\n",
    "\n",
    "                type_str = fix_types(model_type_list[i])\n",
    "                \n",
    "                if type_str not in TYPE_COUNT:\n",
    "                    TYPE_COUNT[type_str] = 0\n",
    "                    FINAL_TYPE_COUNT[type_str] = 0\n",
    "                \n",
    "                TYPE_COUNT[type_str] +=1\n",
    "\n",
    "    print(TYPE_COUNT)\n",
    "    \n",
    "    #remove the unwanted types\n",
    "    temp_removed_keys = []\n",
    "    for rk in REJECT:\n",
    "        for tk in TYPE_COUNT:\n",
    "            \n",
    "            if rk in tk:\n",
    "                print(\"DBG 1\", rk,tk)\n",
    "                temp_removed_keys.append(tk)\n",
    "        for tfm in TYPE_FIX_MAP:\n",
    "            if tfm in rk:\n",
    "                print(\"DBG 2\", tfm,rk)\n",
    "                temp_removed_keys.append(rk)\n",
    "    print('temp_removed_keys',temp_removed_keys)\n",
    "    for trk in temp_removed_keys:\n",
    "        if trk in TYPE_COUNT:\n",
    "            TYPE_COUNT.pop(trk)\n",
    "        if trk in FINAL_TYPE_COUNT:\n",
    "            FINAL_TYPE_COUNT.pop(trk)\n",
    "        if trk in TYPE_FIX_MAP:\n",
    "            TYPE_FIX_MAP.pop(trk)\n",
    "    \n",
    "    TYPE_COUNT = {k: v for k, v in sorted(TYPE_COUNT.items(), key=lambda item: item[1] , reverse=True)}\n",
    "\n",
    "\n",
    "    ti = 0\n",
    "    for key,val in TYPE_COUNT.items():\n",
    "        TYPE_MAPPING[key]  = ti\n",
    "        ti+=1\n",
    "\n",
    "    #make the probability\n",
    "    for key in TYPE_COUNT.keys():\n",
    "        print(MAX_TYPE_SAMPLE , TYPE_COUNT[key] ,key)\n",
    "        TYPE_PROBABILITY[key] = MAX_TYPE_SAMPLE/TYPE_COUNT[key]\n",
    "    \n",
    "\n",
    "    print(TYPE_PROBABILITY)\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    #######################################\n",
    "    \n",
    "\n",
    "    #fill dataset with that probability\n",
    "    for pkl_path,oi, user_proj_name in all_bins_paths:\n",
    "        \n",
    "        bin_numeric_value = sum(bytearray(user_proj_name,'utf-8') )%10\n",
    "        \n",
    "        \n",
    "        with open(pkl_path, 'rb') as file:\n",
    "            try:\n",
    "                model_input_list, model_type_list = pickle.load(file)\n",
    "            except Exception as e:\n",
    "#                 print('err: ',e)\n",
    "                pass\n",
    "            for i in range(len(model_input_list)):\n",
    "                \n",
    "                try:\n",
    "                    type_str = TYPE_FIX_MAP[model_type_list[i]]\n",
    "                    type_label = TYPE_MAPPING[type_str]\n",
    "\n",
    "                    backward_slice , target_slice, forward_slice = model_input_list[i]\n",
    "\n",
    "                    if random.random()<=TYPE_PROBABILITY[type_str]:\n",
    "                        \n",
    "                        if bin_numeric_value>7:\n",
    "                            \n",
    "                            \n",
    "                            ALL_INPUT_LIST.append(backward_slice + target_slice + forward_slice)\n",
    "                            ALL_INPUT_SLICE_INFO.append([len(backward_slice) , len(target_slice) , len(forward_slice)])\n",
    "                            ALL_LABEL_LIST.append(type_label)\n",
    "                            ALL_OPT_LIST.append(oi)\n",
    "                            VAL_SAMPLE_COUNT = VAL_SAMPLE_COUNT + 1\n",
    "                        else:\n",
    "                            \n",
    "                            ALL_INPUT_LIST.insert( 0,  backward_slice + target_slice + forward_slice)\n",
    "                            ALL_INPUT_SLICE_INFO.insert( 0 , [len(backward_slice) , len(target_slice) , len(forward_slice)])\n",
    "                            ALL_LABEL_LIST.insert( 0 , type_label)\n",
    "                            ALL_OPT_LIST.insert(0,oi)\n",
    "                            TRAIN_SAMPLE_COUNT=TRAIN_SAMPLE_COUNT+1\n",
    "\n",
    "\n",
    "                        FINAL_TYPE_COUNT[type_str]+=1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "#                       print('err: _',e)\n",
    "#                       print(traceback.print_exc())\n",
    "\n",
    "make_dataset()\n",
    "print(VAL_SAMPLE_COUNT , TRAIN_SAMPLE_COUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ff019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp_path = 'data/typedata'+EXPERIMENT_NAME+'.ignore.pkl'\n",
    "# with open(data_temp_path, 'wb') as f:\n",
    "#     pickle.dump([ALL_OPT_LIST,VAL_SAMPLE_COUNT , TRAIN_SAMPLE_COUNT,TYPE_COUNT , TYPE_MAPPING ,ALL_INPUT_LIST, ALL_INPUT_SLICE_INFO ,ALL_LABEL_LIST,   FINAL_TYPE_COUNT ], f)\n",
    "    \n",
    "    \n",
    "with open(data_temp_path, 'rb') as file:\n",
    "    ALL_OPT_LIST,VAL_SAMPLE_COUNT , TRAIN_SAMPLE_COUNT,TYPE_COUNT , TYPE_MAPPING ,ALL_INPUT_LIST, ALL_INPUT_SLICE_INFO ,ALL_LABEL_LIST,     FINAL_TYPE_COUNT  = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f9c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620602 {'int': 20066, '*char': 16735, 'structure': 19804, 'long int': 19913, '*int': 18824, 'array_int': 19395, 'char': 18415, 'array_char': 19099, '*structure': 18383, 'short int': 19930, 'array_structure': 13174, 'float': 19809, 'double': 20057, 'unsigned int': 20016, '*double': 18793, 'long unsigned int': 20058, '*float': 13052, 'array_float': 19481, 'long long int': 20190, 'array_double': 19633, 'array_short int': 5046, 'short unsigned int': 14611, '*unsigned int': 10491, 'long long unsigned int': 20046, '*long int': 11943, 'signed char': 19888, '*long long unsigned int': 877, 'array_unsigned int': 19888, 'array_long long int': 10039, '*long unsigned int': 3797, 'array_long long unsigned int': 740, 'array_long int': 12517, 'array_long unsigned int': 852, '*short int': 4145, '*long long int': 4970, '*short unsigned int': 1763, 'array_short unsigned int': 647, '*signed char': 19908, 'array_signed char': 19898, '*long double': 4, 'long double': 122, 'unsigned long int': 20239, 'unsigned long long int': 2368, '*void': 19871, 'array_unsigned long long int': 8253, 'unsigned short int': 6937, 'array_unsigned short int': 2258, '*unsigned long int': 1346, 'array_unsigned long int': 1668, '*unsigned short int': 443, '*unsigned long long int': 197, 'array_long double': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(ALL_INPUT_LIST), FINAL_TYPE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a256faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 5\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = BertTokenizer.from_pretrained(\"./multytask-tokenizer_optim_3_d4\")\n",
    "\n",
    "#https://github.com/huggingface/tokenizers/issues/247\n",
    "\n",
    "mask_token_id, look_token_id, eoi_token_id = tokenizer.encode('[MASK] [LOOK] [EOI]')[1:-1]\n",
    "\n",
    "print(mask_token_id, look_token_id, eoi_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa991f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE :  3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "print('VOCAB SIZE : ', (tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c7d400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/raisul/models/pretrain_100k_O2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"/home/raisul/models/pretrain_100k_O2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "#TODO*** USE PRETRAINED\n",
    "# PATH, local_files_only=True #/home/raisul/models/pretrain_100k #bert-base-uncased\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/raisul/models/pretrain_100k_O2\", num_labels=len(TYPE_MAPPING.items()))\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH  + EXPERIMENT_NAME, num_labels=len(TYPE_MAPPING.items()))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88bd9a8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize input text\n",
    "# inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
    "# inputs = tokenizer(ALL_INPUT_LIST, max_length= MAX_TOKEN_SIZE,padding='max_length', truncation=True , return_tensors='pt')\n",
    "# print(inputs.keys())\n",
    "\n",
    "# labels = ALL_LABEL_LIST.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70219d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs.keys())\n",
    "# print(inputs.token_type_ids)\n",
    "# inputs.token_type_ids[0][0] =1\n",
    "# print(inputs.token_type_ids)\n",
    "# #TODO set token type ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd332543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinaryDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings):\n",
    "#         self.encodings = encodings\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#     def __len__(self):\n",
    "#         return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b2d13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        tokenized_text = (self.tokenizer(text , max_length= MAX_TOKEN_SIZE,padding='max_length', truncation=True , return_tensors='pt')).to(device)\n",
    "        \n",
    "        # Convert tokens to input IDs\n",
    "#         input_ids = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        \n",
    "        # Create input tensors\n",
    "#         input_ids = tokenized_text['input_ids']  #torch.tensor(input_ids)\n",
    "        label = torch.tensor([label]).to(device)\n",
    "        \n",
    "        return tokenized_text, label\n",
    "        \n",
    "#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6640b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = BinaryDataset(ALL_INPUT_LIST, ALL_LABEL_LIST,tokenizer)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# validation_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "\n",
    "# Created using indices from 0 to train_size.\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(TRAIN_SAMPLE_COUNT))\n",
    "\n",
    "# Created using indices from train_size to train_size + test_size.\n",
    "validation_dataset = torch.utils.data.Subset(dataset, range(TRAIN_SAMPLE_COUNT, TRAIN_SAMPLE_COUNT + VAL_SAMPLE_COUNT))\n",
    "\n",
    "\n",
    "# train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "\n",
    "\n",
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE ,shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eac9d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118203, 118203, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SAMPLE_COUNT,  VAL_SAMPLE_COUNT\n",
    "len(validation_dataset),VAL_SAMPLE_COUNT, TRAIN_SAMPLE_COUNT+VAL_SAMPLE_COUNT- len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "492ad250",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/typedata_dataset'+EXPERIMENT_NAME+'.ignore.pkl'\n",
    "with open(dataset_path, 'wb') as f:\n",
    "    pickle.dump([train_dataset, validation_dataset], f)\n",
    "    \n",
    "    \n",
    "with open(dataset_path, 'rb') as file:\n",
    "    train_dataset, validation_dataset  = pickle.load(file)    \n",
    "    \n",
    "    \n",
    "# train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE ,shuffle=True) \n",
    "# validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6496e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3941, 16747)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_loader), len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44653e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_task2_metrices, global_task1_metrices ,v_global_task1_metrices, v_global_task2_metrices\n",
    "\n",
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graph(task1_metrices,   v_task1_metrices, label = \"TypeGraph\" ):\n",
    "    \n",
    "    plt.ioff()\n",
    "\n",
    "    font_size = 10\n",
    "    x_labels = [ i for i in range(len(task1_metrices)) ]\n",
    "    \n",
    "    task1_f1 = [ i['f1'] for i in task1_metrices ]\n",
    "\n",
    "    \n",
    "    v_task1_f1 = [ i['f1'] for i in v_task1_metrices ]\n",
    "\n",
    "    \n",
    "\n",
    "    plt.ylabel(' F1 ',fontsize=font_size)\n",
    "    plt.plot(x_labels, task1_f1 , 'r') \n",
    "\n",
    "    print(x_labels, v_task1_f1 )\n",
    "    plt.plot(x_labels, v_task1_f1 , 'r' , linestyle = '--') \n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Epoch\", fontsize=font_size)\n",
    "    plt.title(label,fontsize=font_size)\n",
    "    plt.legend([' Type Train',  'Type Val' ], loc='upper left')\n",
    "    \n",
    "    plt.savefig('./output/'+label+'_f1.pdf')\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "    \n",
    "    ################################\n",
    "    ################# LOSS #########\n",
    "    ################################\n",
    "    \n",
    "    task1_loss = [ i['loss'] for i in task1_metrices ]\n",
    "\n",
    "    \n",
    "    v_task1_loss = [ i['loss'] for i in v_task1_metrices ]\n",
    "    \n",
    "\n",
    "    plt.ylabel(' LOSS ',fontsize=font_size)\n",
    "    plt.plot(x_labels, task1_loss , 'r') \n",
    "    \n",
    "    plt.plot(x_labels, v_task1_loss , 'r' , linestyle = '--') \n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Epoch\", fontsize=font_size)\n",
    "    plt.title(label,fontsize=font_size)\n",
    "    plt.legend([' Type Train',  'Type Val'], loc='upper left')\n",
    "    \n",
    "    plt.savefig('./output/'+label+'_loss.pdf')\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5142580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix,ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, label='confusion_matrix'):\n",
    "\n",
    "    class_labels = list(TYPE_MAPPING.keys())  \n",
    "    class_labels = [c for c in class_labels]\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels ) \n",
    "    \n",
    "    conf_per_class = cm.diagonal()/cm.sum(axis=1)\n",
    "    average_acc = sum([i for i in conf_per_class  if not math.isnan(i)] )/len(conf_per_class)\n",
    "    \n",
    "    print('CONFUSION PER CLASS' , conf_per_class,average_acc)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    \n",
    "    res = sns.heatmap(cm,\n",
    "            annot=True , cmap=\"Blues\" , fmt='g' , xticklabels=class_labels,linewidths = .01,\n",
    "                      yticklabels=class_labels,linecolor=\"Gray\")\n",
    "    for _, spine in res.spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1)\n",
    "    \n",
    "    plt.ylabel('Actual',fontsize=13)\n",
    "    plt.xlabel('Prediction',fontsize=13)\n",
    "    plt.title('Confusion Matrix',fontsize=17)\n",
    "    plt.savefig('./output/'+label+'_conf.pdf',dpi=200)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    f1_score_per_class = f1_score(true_labels, predicted_labels, average=None)\n",
    "    print(f1_score_per_class,class_labels)\n",
    "    f1_scores_with_labels = {label:score for label,score in zip(class_labels, f1_score_per_class)}\n",
    "    print(f1_scores_with_labels)\n",
    "# plot_confusion_matrix(v_ground_truth_s , v_prediction_s ,label=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4054b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "  #--optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    " # --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "  #--dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "\n",
    "#     ( params: typing.Iterable[torch.nn.parameter.Parameter]lr: float = 0.001betas: typing.Tuple[float, \n",
    "# float] = (0.9, 0.999)eps: float = 1e-06weight_decay: float = 0.0correct_bias: bool = Trueno_deprecation_warning: bool = False )\n",
    "    \n",
    "# initialize optimizer\n",
    "optim = AdamW( model.parameters() , lr=1e-5, eps = 1e-6, betas=(0.9,0.98), weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84807a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model ,data_loop, is_training = False):\n",
    "    \n",
    "    prediction_s, ground_truth_s = [], []\n",
    "    losses = []\n",
    "\n",
    "    for N,batch in enumerate(data_loop):\n",
    "\n",
    "        # Forward pass\n",
    "        if is_training == True:\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        batch_input, batch_labels = batch\n",
    "        if len(batch_labels)<BATCH_SIZE:\n",
    "            continue\n",
    "        batch_input_ids= batch_input['input_ids']\n",
    "        batch_attention_mask=batch_input['attention_mask']\n",
    "        batch_token_type_ids =batch_input['token_type_ids']\n",
    "\n",
    "        outputs = model(input_ids=batch_input_ids.squeeze(),\n",
    "                        attention_mask=batch_attention_mask.squeeze(),\n",
    "                        token_type_ids=batch_token_type_ids.squeeze(),\n",
    "                        labels=batch_labels )\n",
    "        \n",
    "#\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "\n",
    "\n",
    "        prediction_s.extend(predictions.detach().cpu().numpy().flatten())\n",
    "        ground_truth_s.extend(batch_labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "        if is_training == True:\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "        # print relevant info to progress bar\n",
    "        data_loop.set_description(f'Epoch {ecpoch}')\n",
    "        data_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    ###### Training Scores\n",
    "    accuracy = accuracy_score(ground_truth_s, prediction_s)    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ground_truth_s,prediction_s,average='weighted')\n",
    "    metrices = {'accuracy':accuracy ,\n",
    "                      'precision':precision, \n",
    "                      'recall':recall, \n",
    "                      'f1':f1,\n",
    "                      'loss': (sum(losses) / len(losses))}\n",
    "    \n",
    "\n",
    "\n",
    "    return metrices , prediction_s, ground_truth_s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  54%|███████████████████████████▉                        | 8996/16747 [1:03:20<54:54,  2.35it/s, loss=0.916]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "global_metrices = []\n",
    "v_global_metrices = []\n",
    "\n",
    "\n",
    "for ecpoch in range(EPOCHS):\n",
    "    \n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    model.train()\n",
    "    metrices,prediction_s, ground_truth_s  = training_loop(model ,train_loop, is_training = True)\n",
    "    print('Training metrices: ',metrices)\n",
    "    global_metrices.append(metrices)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        v_metrices, v_prediction_s, v_ground_truth_s  = training_loop(model ,validation_loop, is_training = False)\n",
    "        plot_confusion_matrix(v_ground_truth_s , v_prediction_s ,label=EXPERIMENT_NAME)\n",
    "        print('v_metrices: ',v_metrices)\n",
    "        v_global_metrices.append(v_metrices)\n",
    "    plot_graph(global_metrices,v_global_metrices , label = EXPERIMENT_NAME)\n",
    "    print('SAVING MODEL @ ',MODEL_SAVE_PATH +EXPERIMENT_NAME)\n",
    "#     model.save_pretrained(MODEL_SAVE_PATH +EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ddaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained( MODEL_SAVE_PATH  + EXPERIMENT_NAME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Alhamdulillah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce0879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae93b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
