{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Instruction Prediction Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nahid/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gitwipe 4\n",
      "gitps 147\n",
      "gitview 140\n",
      "gitfm 341\n",
      "gitwhich 6\n",
      "gitkeys 4\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dir_path = \"./data/binaries/\"\n",
    "dir_file_list = os.listdir(data_dir_path)\n",
    "\n",
    "with open('./data/instruction_clusters.txt', 'w') as data_file:\n",
    "    for filename in dir_file_list:\n",
    "        filePath = os.path.join(data_dir_path,filename)\n",
    "\n",
    "        fh = open(filePath, 'rb')\n",
    "        bin_bytearray = bytearray(fh.read())\n",
    "        \n",
    "        with open(filePath, 'rb') as f:\n",
    "            elf = ELFFile(f)\n",
    "            dwarfinfo = elf.get_dwarf_info()\n",
    "            aranges = dwarfinfo.get_aranges()\n",
    "            print(filename, len(aranges.entries))\n",
    "            for arange in aranges.entries:\n",
    "\n",
    "                entry = arange.begin_addr\n",
    "                exit  = arange.begin_addr + arange.length\n",
    "                ops = bin_bytearray[entry: exit]\n",
    "\n",
    "                md = Cs(CS_ARCH_X86, CS_MODE_64)\n",
    "                md.detail = True\n",
    "                for inst in md.disasm(ops, entry):\n",
    "\n",
    "                    data_file.write(inst.mnemonic+\" \"+inst.op_str+\";\")\n",
    "                data_file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./binary-tokenizer\")\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim = ';'\n",
    "with open('./data/instruction_clusters.txt', 'r') as fp:\n",
    "    text = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split sentences into consecutive, and non-consecutive sequences.\n",
    "\n",
    "We have to deal with edge-cases too - for example where there is only a single sentence within a paragraph as with the three examples above (in comparison to below where we can easily split into multiple sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text[51].split(delim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assign a 50% probability of using the genuine next sentence, and 50% probability of using another random sentence.\n",
    "\n",
    "To make this simpler, we'll create a *'bag'* of individual sentences to pull from when selecting a random sentence B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49783\n"
     ]
    }
   ],
   "source": [
    "bag = [instruction for instruction_cluster in text for instruction in instruction_cluster.split(delim)  if instruction!= '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'mov rdx, qword ptr [rip + 0x2d98]',\n",
       " 'mov rax, qword ptr [rip + 0x2d81]',\n",
       " 'lea rcx, [rip + 0xd62]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov edi, 1',\n",
       " 'call 0x1170',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x20',\n",
       " 'mov dword ptr [rbp - 0x14], edi',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'mov edx, 1',\n",
       " 'mov esi, 0',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1180',\n",
       " 'mov qword ptr [rbp - 0x10], rax',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'mov edx, 2',\n",
       " 'mov esi, 0',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1180',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov rcx, qword ptr [rbp - 0x10]',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'mov edx, 0',\n",
       " 'mov rsi, rcx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1180',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x40',\n",
       " 'mov qword ptr [rbp - 0x38], rdi',\n",
       " 'mov rax, qword ptr [rbp - 0x38]',\n",
       " 'mov esi, 2',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1160',\n",
       " 'mov dword ptr [rbp - 0x2c], eax',\n",
       " 'cmp dword ptr [rbp - 0x2c], -1',\n",
       " 'jne 0x137a',\n",
       " 'mov rdx, qword ptr [rip + 0x2cdf]',\n",
       " 'mov rax, qword ptr [rip + 0x2cc8]',\n",
       " 'mov rcx, qword ptr [rbp - 0x38]',\n",
       " 'lea rsi, [rip + 0xcbd]',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov eax, 1',\n",
       " 'jmp 0x14cd',\n",
       " 'mov eax, dword ptr [rbp - 0x2c]',\n",
       " 'mov edi, eax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x12c0',\n",
       " 'mov qword ptr [rbp - 0x18], rax',\n",
       " 'cmp qword ptr [rbp - 0x18], 0',\n",
       " 'jne 0x139e',\n",
       " 'mov eax, 0',\n",
       " 'jmp 0x14cd',\n",
       " 'mov edi, 0x10000',\n",
       " 'call 0x1150',\n",
       " 'mov qword ptr [rbp - 0x10], rax',\n",
       " 'cmp qword ptr [rbp - 0x10], 0',\n",
       " 'jne 0x13e2',\n",
       " 'mov rdx, qword ptr [rip + 0x2c76]',\n",
       " 'mov rax, qword ptr [rip + 0x2c5f]',\n",
       " 'lea rcx, [rip + 0xc78]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov eax, 1',\n",
       " 'jmp 0x14cd',\n",
       " 'mov qword ptr [rbp - 0x28], 0',\n",
       " 'jmp 0x14ab',\n",
       " 'mov rax, qword ptr [rbp - 0x18]',\n",
       " 'sub rax, qword ptr [rbp - 0x28]',\n",
       " 'mov edx, 0x10000',\n",
       " 'cmp rax, rdx',\n",
       " 'cmovg rax, rdx',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov qword ptr [rbp - 0x20], 0',\n",
       " 'jmp 0x1451',\n",
       " 'call 0x1190',\n",
       " 'movsxd rdx, eax',\n",
       " 'imul rdx, rdx, -0x7f7f7f7f',\n",
       " 'shr rdx, 0x20',\n",
       " 'add edx, eax',\n",
       " 'sar edx, 7',\n",
       " 'mov esi, eax',\n",
       " 'sar esi, 0x1f',\n",
       " 'mov ecx, edx',\n",
       " 'sub ecx, esi',\n",
       " 'mov edx, ecx',\n",
       " 'shl edx, 8',\n",
       " 'sub edx, ecx',\n",
       " 'sub eax, edx',\n",
       " 'mov ecx, eax',\n",
       " 'mov rdx, qword ptr [rbp - 0x20]',\n",
       " 'mov rax, qword ptr [rbp - 0x10]',\n",
       " 'add rax, rdx',\n",
       " 'mov edx, ecx',\n",
       " 'mov byte ptr [rax], dl',\n",
       " 'add qword ptr [rbp - 0x20], 1',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'cmp rax, qword ptr [rbp - 8]',\n",
       " 'jl 0x1411',\n",
       " 'mov rdx, qword ptr [rbp - 8]',\n",
       " 'mov rcx, qword ptr [rbp - 0x10]',\n",
       " 'mov eax, dword ptr [rbp - 0x2c]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x10f0',\n",
       " 'cmp qword ptr [rbp - 8], rax',\n",
       " 'je 0x14a3',\n",
       " 'mov rdx, qword ptr [rip + 0x2bb3]',\n",
       " 'mov rax, qword ptr [rip + 0x2b9c]',\n",
       " 'mov rcx, qword ptr [rbp - 0x38]',\n",
       " 'lea rsi, [rip + 0xbd0]',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov eax, 1',\n",
       " 'jmp 0x14cd',\n",
       " 'add qword ptr [rbp - 0x28], 0x10000',\n",
       " 'mov rax, qword ptr [rbp - 0x28]',\n",
       " 'cmp rax, qword ptr [rbp - 0x18]',\n",
       " 'jl 0x13ef',\n",
       " 'mov eax, dword ptr [rbp - 0x2c]',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1100',\n",
       " 'call 0x1140',\n",
       " 'mov eax, 0',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x20',\n",
       " 'mov dword ptr [rbp - 0x14], edi',\n",
       " 'mov qword ptr [rbp - 0x20], rsi',\n",
       " 'mov dword ptr [rbp - 4], 0',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'mov rax, qword ptr [rax]',\n",
       " 'mov qword ptr [rip + 0x2b39], rax',\n",
       " 'cmp dword ptr [rbp - 0x14], 1',\n",
       " 'jg 0x1507',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1289',\n",
       " 'mov edi, 0',\n",
       " 'call 0x1130',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1110',\n",
       " 'mov dword ptr [rbp - 8], 1',\n",
       " 'jmp 0x154c',\n",
       " 'mov eax, dword ptr [rbp - 8]',\n",
       " 'cdqe ',\n",
       " 'lea rdx, [rax*8]',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'add rax, rdx',\n",
       " 'mov rax, qword ptr [rax]',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x131b',\n",
       " 'add dword ptr [rbp - 4], eax',\n",
       " 'add dword ptr [rbp - 8], 1',\n",
       " 'mov eax, dword ptr [rbp - 8]',\n",
       " 'cmp eax, dword ptr [rbp - 0x14]',\n",
       " 'jl 0x1521',\n",
       " 'cmp dword ptr [rbp - 4], 0',\n",
       " 'setne al',\n",
       " 'movzx eax, al',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'mov rax, qword ptr [rip + 0xdd38]',\n",
       " 'test rax, rax',\n",
       " 'je 0x3b2c',\n",
       " 'mov rax, qword ptr [rip + 0xdd2c]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x35b0',\n",
       " 'mov rax, qword ptr [rip + 0xdd25]',\n",
       " 'test rax, rax',\n",
       " 'je 0x3b47',\n",
       " 'mov rax, qword ptr [rip + 0xdd19]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x35b0',\n",
       " 'nop ',\n",
       " 'pop rbp',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'mov eax, dword ptr [rip + 0xde08]',\n",
       " 'movsxd rdx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xdd0e]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3700',\n",
       " 'mov rax, qword ptr [rip + 0xdd32]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xddda]',\n",
       " 'cmp edx, eax',\n",
       " 'jge 0x3b9d',\n",
       " 'mov rax, qword ptr [rip + 0xdd17]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'cdqe ',\n",
       " 'jmp 0x3ba5',\n",
       " 'mov eax, dword ptr [rip + 0xddbd]',\n",
       " 'cdqe ',\n",
       " 'mov rsi, qword ptr [rip + 0xdcfc]',\n",
       " 'mov rcx, qword ptr [rip + 0xdcbd]',\n",
       " 'mov rdx, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3820',\n",
       " 'mov edx, dword ptr [rip + 0xc4c0]',\n",
       " 'mov ecx, dword ptr [rip + 0xc4b6]',\n",
       " 'mov eax, dword ptr [rip + 0xc4b8]',\n",
       " 'mov esi, ecx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x759a',\n",
       " 'mov rax, qword ptr [rip + 0xdca8]',\n",
       " 'mov edx, 0',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " 'mov edx, dword ptr [rip + 0xdd68]',\n",
       " 'mov rcx, qword ptr [rip + 0xdc71]',\n",
       " 'mov rax, qword ptr [rip + 0xdc82]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x912b',\n",
       " 'nop ',\n",
       " 'pop rbp',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'push rbx',\n",
       " 'sub rsp, 8',\n",
       " 'mov eax, dword ptr [rip + 0xdd39]',\n",
       " 'movsxd rdx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xdc3f]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3700',\n",
       " 'mov eax, dword ptr [rip + 0xdd1c]',\n",
       " 'lea ebx, [rax - 2]',\n",
       " 'lea rax, [rip + 0xd3f2]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'cmp ebx, eax',\n",
       " 'jg 0x3c67',\n",
       " 'mov eax, dword ptr [rip + 0xdd00]',\n",
       " 'sub eax, 2',\n",
       " 'cdqe ',\n",
       " 'jmp 0x3c78',\n",
       " 'lea rax, [rip + 0xd3d2]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'cdqe ',\n",
       " 'mov rdx, qword ptr [rip + 0xdbf1]',\n",
       " 'lea rcx, [rdx + 2]',\n",
       " 'mov rdx, rax',\n",
       " 'lea rax, [rip + 0xd3b3]',\n",
       " 'mov rsi, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3820',\n",
       " 'mov edx, dword ptr [rip + 0xc3f2]',\n",
       " 'mov ecx, dword ptr [rip + 0xc3e8]',\n",
       " 'mov eax, dword ptr [rip + 0xc3ea]',\n",
       " 'mov esi, ecx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x759a',\n",
       " 'mov rax, qword ptr [rip + 0xdbd6]',\n",
       " 'mov edx, 0',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " 'mov edx, dword ptr [rip + 0xdc8e]',\n",
       " 'mov rcx, qword ptr [rip + 0xdb97]',\n",
       " 'mov rax, qword ptr [rip + 0xdbb0]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x912b',\n",
       " 'nop ',\n",
       " 'mov rbx, qword ptr [rbp - 8]',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x10',\n",
       " 'mov qword ptr [rbp - 8], rdi',\n",
       " 'mov eax, dword ptr [rip + 0xdc58]',\n",
       " 'movsxd rdx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xdb5e]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3700',\n",
       " 'cmp qword ptr [rbp - 8], 0',\n",
       " 'je 0x3d6e',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xdc26]',\n",
       " 'cmp edx, eax',\n",
       " 'jge 0x3d4e',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'cdqe ',\n",
       " 'jmp 0x3d56',\n",
       " 'mov eax, dword ptr [rip + 0xdc0c]',\n",
       " 'cdqe ',\n",
       " 'mov rcx, qword ptr [rip + 0xdb13]',\n",
       " 'mov rsi, qword ptr [rbp - 8]',\n",
       " 'mov rdx, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3820',\n",
       " 'jmp 0x3dbd',\n",
       " 'mov rax, qword ptr [rip + 0xdb3b]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xdbdb]',\n",
       " 'cmp edx, eax',\n",
       " 'jge 0x3d9c',\n",
       " 'mov rax, qword ptr [rip + 0xdb20]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'cdqe ',\n",
       " 'jmp 0x3da4',\n",
       " 'mov eax, dword ptr [rip + 0xdbbe]',\n",
       " 'cdqe ',\n",
       " 'mov rsi, qword ptr [rip + 0xdb05]',\n",
       " 'mov rcx, qword ptr [rip + 0xdabe]',\n",
       " 'mov rdx, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3820',\n",
       " 'mov edx, dword ptr [rip + 0xc2e5]',\n",
       " 'mov ecx, dword ptr [rip + 0xc2db]',\n",
       " 'mov eax, dword ptr [rip + 0xc2dd]',\n",
       " 'mov esi, ecx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x759a',\n",
       " 'mov rax, qword ptr [rip + 0xdac1]',\n",
       " 'mov edx, 0',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " 'mov eax, dword ptr [rip + 0xdb69]',\n",
       " 'cmp eax, 9',\n",
       " 'jg 0x3e1d',\n",
       " 'mov edx, dword ptr [rip + 0xdb5e]',\n",
       " 'mov rcx, qword ptr [rip + 0xda67]',\n",
       " 'mov rax, qword ptr [rip + 0xda90]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x912b',\n",
       " 'jmp 0x3e5d',\n",
       " 'mov rdx, qword ptr [rip + 0xda4c]',\n",
       " 'mov eax, dword ptr [rip + 0xdb36]',\n",
       " 'sub eax, 1',\n",
       " 'cdqe ',\n",
       " 'sub rax, 0xa',\n",
       " 'add rax, rdx',\n",
       " 'mov byte ptr [rax], 0x20',\n",
       " 'mov eax, dword ptr [rip + 0xdb21]',\n",
       " 'sub eax, 0xa',\n",
       " 'mov edx, eax',\n",
       " 'mov rcx, qword ptr [rip + 0xda25]',\n",
       " 'mov rax, qword ptr [rip + 0xda4e]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x912b',\n",
       " 'nop ',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x20',\n",
       " 'mov dword ptr [rbp - 0x14], edi',\n",
       " 'mov dword ptr [rbp - 4], 9',\n",
       " 'cmp dword ptr [rbp - 0x14], 0',\n",
       " 'js 0x3eb5',\n",
       " 'mov dword ptr [rbp - 8], 0',\n",
       " 'jmp 0x3eaf',\n",
       " 'mov eax, dword ptr [rbp - 8]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 4',\n",
       " 'mov rdx, rax',\n",
       " 'lea rax, [rip + 0xc2b4]',\n",
       " 'mov eax, dword ptr [rdx + rax]',\n",
       " 'cmp dword ptr [rbp - 0x14], eax',\n",
       " 'jne 0x3eab',\n",
       " 'mov eax, dword ptr [rbp - 8]',\n",
       " 'mov dword ptr [rip + 0xda0f], eax',\n",
       " 'jmp 0x3eb5',\n",
       " 'add dword ptr [rbp - 8], 1',\n",
       " 'cmp dword ptr [rbp - 8], 0x21',\n",
       " 'jle 0x3e85',\n",
       " 'mov eax, dword ptr [rip + 0xdaa5]',\n",
       " 'cmp dword ptr [rbp - 4], eax',\n",
       " 'jge 0x3f3b',\n",
       " 'mov edx, dword ptr [rip + 0xc1e2]',\n",
       " 'mov eax, dword ptr [rip + 0xc1e0]',\n",
       " 'mov esi, 7',\n",
       " 'mov edi, eax',\n",
       " 'call 0x759a',\n",
       " 'mov eax, dword ptr [rip + 0xda82]',\n",
       " 'sub eax, dword ptr [rbp - 4]',\n",
       " 'lea edx, [rax - 1]',\n",
       " 'mov rax, qword ptr [rip + 0xd9b5]',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " 'mov eax, dword ptr [rip + 0xd9ba]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 4',\n",
       " 'mov rdx, rax',\n",
       " 'lea rax, [rip + 0xc232]',\n",
       " 'lea rcx, [rdx + rax]',\n",
       " 'mov rax, qword ptr [rip + 0xd987]',\n",
       " 'mov edx, dword ptr [rbp - 4]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x912b',\n",
       " 'mov rax, qword ptr [rip + 0xd972]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x91cd',\n",
       " 'mov eax, dword ptr [rip + 0xda1f]',\n",
       " 'lea edx, [rax - 1]',\n",
       " 'mov rax, qword ptr [rip + 0xd955]',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " 'nop ',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x20',\n",
       " 'mov eax, 0',\n",
       " 'call 0x7e66',\n",
       " 'mov qword ptr [rbp - 0x20], rax',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'mov qword ptr [rbp - 0x18], rax',\n",
       " 'cmp qword ptr [rbp - 0x18], 0',\n",
       " 'je 0x40d6',\n",
       " 'mov rax, qword ptr [rbp - 0x18]',\n",
       " 'lea rdx, [rax - 1]',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'cmp al, 7',\n",
       " 'je 0x40d6',\n",
       " 'mov eax, 0',\n",
       " 'call 0x7e66',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x68d7',\n",
       " 'mov qword ptr [rbp - 0x10], rax',\n",
       " 'mov rax, qword ptr [rbp - 0x10]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'sub rax, -0x80',\n",
       " 'mov rdi, rax',\n",
       " 'call 0xba00',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov rdx, qword ptr [rbp - 0x10]',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'lea rcx, [rip + 0x80e6]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x3980',\n",
       " 'mov eax, dword ptr [rip + 0xd95f]',\n",
       " 'movsxd rdx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xd865]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3700',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xd934]',\n",
       " 'cmp edx, eax',\n",
       " 'jge 0x4040',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'cdqe ',\n",
       " 'jmp 0x4048',\n",
       " 'mov eax, dword ptr [rip + 0xd91a]',\n",
       " 'cdqe ',\n",
       " 'mov rcx, qword ptr [rip + 0xd821]',\n",
       " 'mov rsi, qword ptr [rbp - 8]',\n",
       " 'mov rdx, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3820',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0xb878',\n",
       " 'mov edx, 1',\n",
       " 'mov esi, 7',\n",
       " 'mov edi, 1',\n",
       " 'call 0x759a',\n",
       " 'mov rax, qword ptr [rip + 0xd81b]',\n",
       " 'mov edx, 0',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " 'mov edx, dword ptr [rip + 0xd8c3]',\n",
       " 'mov rcx, qword ptr [rip + 0xd7cc]',\n",
       " 'mov rax, qword ptr [rip + 0xd7f5]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x912b',\n",
       " 'mov eax, 0',\n",
       " 'call 0x75d0',\n",
       " 'mov eax, 0',\n",
       " 'call 0x6ac8',\n",
       " 'mov edi, 1',\n",
       " 'call 0x39d0',\n",
       " 'jmp 0x40e0',\n",
       " 'mov eax, 0',\n",
       " 'call 0x75d0',\n",
       " 'mov edi, 0',\n",
       " 'mov eax, 0',\n",
       " 'call 0x3cf2',\n",
       " 'mov edi, 0xffffffff',\n",
       " 'mov eax, 0',\n",
       " 'call 0x3e60',\n",
       " 'mov eax, 0',\n",
       " 'call 0x6ac8',\n",
       " 'nop ',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x10',\n",
       " 'mov dword ptr [rbp - 4], 0',\n",
       " 'jmp 0x415c',\n",
       " 'mov rdx, qword ptr [rip + 0xd739]',\n",
       " 'mov eax, dword ptr [rbp - 4]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 3',\n",
       " 'add rax, rdx',\n",
       " 'mov rax, qword ptr [rax]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0xb878',\n",
       " 'mov rdx, qword ptr [rip + 0xd71b]',\n",
       " 'mov eax, dword ptr [rbp - 4]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 3',\n",
       " 'add rax, rdx',\n",
       " 'mov qword ptr [rax], 0',\n",
       " 'add dword ptr [rbp - 4], 1',\n",
       " 'mov eax, dword ptr [rip + 0xceaa]',\n",
       " 'cmp dword ptr [rbp - 4], eax',\n",
       " 'jl 0x4120',\n",
       " 'nop ',\n",
       " 'nop ',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x30',\n",
       " 'mov qword ptr [rbp - 0x28], rdi',\n",
       " 'mov qword ptr [rbp - 0x30], rsi',\n",
       " 'mov rdx, qword ptr [rbp - 0x28]',\n",
       " 'mov rax, qword ptr [rbp - 0x30]',\n",
       " 'mov esi, 0x7ff',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x37a0',\n",
       " 'mov qword ptr [rbp - 0x10], rax',\n",
       " 'cmp qword ptr [rbp - 0x10], 0',\n",
       " 'jne 0x41a6',\n",
       " 'mov eax, 0',\n",
       " 'jmp 0x41fc',\n",
       " 'mov rax, qword ptr [rbp - 0x30]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'sub rax, 1',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov rdx, qword ptr [rbp - 0x30]',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'cmp al, 0xa',\n",
       " 'jne 0x41dc',\n",
       " 'mov rdx, qword ptr [rbp - 0x30]',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'add rax, rdx',\n",
       " 'mov byte ptr [rax], 0',\n",
       " 'jmp 0x41f8',\n",
       " 'nop ',\n",
       " 'mov rax, qword ptr [rbp - 0x28]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3740',\n",
       " 'mov dword ptr [rbp - 0x14], eax',\n",
       " 'cmp dword ptr [rbp - 0x14], 0xa',\n",
       " 'je 0x41f8',\n",
       " 'cmp dword ptr [rbp - 0x14], -1',\n",
       " 'jne 0x41dd',\n",
       " 'mov rax, qword ptr [rbp - 0x10]',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x20',\n",
       " 'mov qword ptr [rbp - 0x18], rdi',\n",
       " 'lea rax, [rip + 0xce2b]',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov rax, qword ptr [rbp - 0x18]',\n",
       " 'lea rdx, [rip + 0xce1c]',\n",
       " 'mov rsi, rdx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x416b',\n",
       " 'test rax, rax',\n",
       " 'jne 0x4243',\n",
       " 'mov eax, 0xffffffff',\n",
       " 'jmp 0x42f8',\n",
       " 'lea rax, [rip + 0x7e98]',\n",
       " 'mov rsi, rax',\n",
       " 'lea rax, [rip + 0xcdec]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x39f0',\n",
       " 'test rax, rax',\n",
       " 'jne 0x426b',\n",
       " 'mov eax, 0xffffffff',\n",
       " 'jmp 0x42f8',\n",
       " 'mov dword ptr [rbp - 0xc], 0',\n",
       " 'jmp 0x4279',\n",
       " 'add qword ptr [rbp - 8], 1',\n",
       " 'call 0x3a10',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'movsx rax, al',\n",
       " 'add rax, rax',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, word ptr [rax]',\n",
       " 'movzx eax, ax',\n",
       " 'and eax, 0x2000',\n",
       " 'test eax, eax',\n",
       " 'jne 0x4274',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'mov edx, 3',\n",
       " 'lea rcx, [rip + 0x7e31]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3790',\n",
       " 'test eax, eax',\n",
       " 'jne 0x42ca',\n",
       " 'mov eax, dword ptr [rbp - 0xc]',\n",
       " 'jmp 0x42f8',\n",
       " 'add qword ptr [rbp - 8], 1',\n",
       " 'call 0x3a10',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'movsx rax, al',\n",
       " 'add rax, rax',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, word ptr [rax]',\n",
       " 'movzx eax, ax',\n",
       " 'and eax, 0x2000',\n",
       " 'test eax, eax',\n",
       " 'je 0x42c5',\n",
       " 'add dword ptr [rbp - 0xc], 1',\n",
       " 'jmp 0x4279',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'push rbx',\n",
       " 'sub rsp, 0xb8',\n",
       " 'mov dword ptr [rbp - 0xb4], edi',\n",
       " 'mov rax, qword ptr fs:[0x28]',\n",
       " 'mov qword ptr [rbp - 0x18], rax',\n",
       " 'xor eax, eax',\n",
       " 'mov rdx, qword ptr [rip + 0xd53a]',\n",
       " 'mov eax, dword ptr [rbp - 0xb4]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 3',\n",
       " 'add rax, rdx',\n",
       " 'mov rax, qword ptr [rax]',\n",
       " 'mov qword ptr [rbp - 0xa8], rax',\n",
       " 'mov dword ptr [rbp - 0xb0], 0',\n",
       " 'jmp 0x43ba',\n",
       " 'add qword ptr [rbp - 0xa8], 1',\n",
       " 'call 0x3a10',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov rax, qword ptr [rbp - 0xa8]',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'movsx rax, al',\n",
       " 'add rax, rax',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, word ptr [rax]',\n",
       " 'movzx eax, ax',\n",
       " 'and eax, 0x2000',\n",
       " 'test eax, eax',\n",
       " 'jne 0x434b',\n",
       " 'jmp 0x4388',\n",
       " 'add qword ptr [rbp - 0xa8], 1',\n",
       " 'call 0x3a10',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov rax, qword ptr [rbp - 0xa8]',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'movsx rax, al',\n",
       " 'add rax, rax',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, word ptr [rax]',\n",
       " 'movzx eax, ax',\n",
       " 'and eax, 0x2000',\n",
       " 'test eax, eax',\n",
       " 'je 0x4380',\n",
       " 'add dword ptr [rbp - 0xb0], 1',\n",
       " 'mov eax, dword ptr [rip + 0xcc50]',\n",
       " 'cmp dword ptr [rbp - 0xb0], eax',\n",
       " 'jl 0x4353',\n",
       " 'mov dword ptr [rbp - 0xb0], 0',\n",
       " 'jmp 0x43dc',\n",
       " 'add qword ptr [rbp - 0xa8], 1',\n",
       " 'call 0x3a10',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov rax, qword ptr [rbp - 0xa8]',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'movsx rax, al',\n",
       " 'add rax, rax',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, word ptr [rax]',\n",
       " 'movzx eax, ax',\n",
       " 'and eax, 0x2000',\n",
       " 'test eax, eax',\n",
       " 'jne 0x43d4',\n",
       " 'jmp 0x4437',\n",
       " 'mov rax, qword ptr [rbp - 0xa8]',\n",
       " 'lea rdx, [rax + 1]',\n",
       " 'mov qword ptr [rbp - 0xa8], rdx',\n",
       " 'mov edx, dword ptr [rbp - 0xb0]',\n",
       " 'lea ecx, [rdx + 1]',\n",
       " 'mov dword ptr [rbp - 0xb0], ecx',\n",
       " 'movzx ecx, byte ptr [rax]',\n",
       " 'movsxd rax, edx',\n",
       " 'mov byte ptr [rbp + rax - 0xa0], cl',\n",
       " 'call 0x3a10',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov rax, qword ptr [rbp - 0xa8]',\n",
       " 'movzx eax, byte ptr [rax]',\n",
       " 'movsx rax, al',\n",
       " 'add rax, rax',\n",
       " 'add rax, rdx',\n",
       " 'movzx eax, word ptr [rax]',\n",
       " 'movzx eax, ax',\n",
       " 'and eax, 0x2000',\n",
       " 'test eax, eax',\n",
       " 'je 0x4409',\n",
       " 'mov eax, dword ptr [rbp - 0xb0]',\n",
       " 'cdqe ',\n",
       " 'mov byte ptr [rbp + rax - 0xa0], 0',\n",
       " 'lea rax, [rbp - 0xa0]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3960',\n",
       " 'mov dword ptr [rbp - 0xac], eax',\n",
       " 'cmp dword ptr [rbp - 0xac], 0',\n",
       " 'je 0x44cb',\n",
       " 'mov eax, dword ptr [rip + 0xd422]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 4',\n",
       " 'mov rdx, rax',\n",
       " 'lea rax, [rip + 0xbca6]',\n",
       " 'mov ebx, dword ptr [rdx + rax]',\n",
       " 'lea rax, [rbp - 0xa0]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3960',\n",
       " 'mov esi, ebx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x3830',\n",
       " 'test eax, eax',\n",
       " 'sete al',\n",
       " 'movzx eax, al',\n",
       " 'jmp 0x44d0',\n",
       " 'mov eax, 0xffffffff',\n",
       " 'mov rdx, qword ptr [rbp - 0x18]',\n",
       " 'sub rdx, qword ptr fs:[0x28]',\n",
       " 'je 0x44e4',\n",
       " 'call 0x3680',\n",
       " 'mov rbx, qword ptr [rbp - 8]',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'push rbx',\n",
       " 'sub rsp, 0x838',\n",
       " 'mov qword ptr [rbp - 0x838], rdi',\n",
       " 'mov rax, qword ptr fs:[0x28]',\n",
       " 'mov qword ptr [rbp - 0x18], rax',\n",
       " 'xor eax, eax',\n",
       " 'mov dword ptr [rbp - 0x824], 0',\n",
       " 'jmp 0x4578',\n",
       " 'mov eax, dword ptr [rbp - 0x824]',\n",
       " 'add eax, 1',\n",
       " 'cdqe ',\n",
       " 'lea rdx, [rax*8]',\n",
       " 'mov rax, qword ptr [rip + 0xd32a]',\n",
       " 'mov rsi, rdx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0xba20',\n",
       " 'mov qword ptr [rip + 0xd318], rax',\n",
       " 'mov rdx, qword ptr [rip + 0xd311]',\n",
       " 'mov eax, dword ptr [rbp - 0x824]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 3',\n",
       " 'lea rbx, [rdx + rax]',\n",
       " 'lea rax, [rbp - 0x820]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0xbb30',\n",
       " 'mov qword ptr [rbx], rax',\n",
       " 'add dword ptr [rbp - 0x824], 1',\n",
       " 'lea rdx, [rbp - 0x820]',\n",
       " 'mov rax, qword ptr [rbp - 0x838]',\n",
       " 'mov rsi, rdx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x416b',\n",
       " 'test rax, rax',\n",
       " 'jne 0x451c',\n",
       " 'mov eax, dword ptr [rbp - 0x824]',\n",
       " 'mov dword ptr [rip + 0xca65], eax',\n",
       " 'nop ',\n",
       " 'mov rax, qword ptr [rbp - 0x18]',\n",
       " 'sub rax, qword ptr fs:[0x28]',\n",
       " 'je 0x45bc',\n",
       " 'call 0x3680',\n",
       " 'mov rbx, qword ptr [rbp - 8]',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'push rbx',\n",
       " 'sub rsp, 0x28',\n",
       " 'mov dword ptr [rbp - 0x24], edi',\n",
       " 'mov dword ptr [rbp - 0x28], esi',\n",
       " 'mov rdx, qword ptr [rip + 0xd284]',\n",
       " 'mov eax, dword ptr [rbp - 0x24]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 3',\n",
       " 'add rax, rdx',\n",
       " 'mov rax, qword ptr [rax]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'mov dword ptr [rbp - 0x18], eax',\n",
       " 'mov eax, dword ptr [rip + 0xd364]',\n",
       " 'sub eax, 2',\n",
       " 'mov dword ptr [rbp - 0x14], eax',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'cmp eax, dword ptr [rbp - 0x18]',\n",
       " 'jl 0x4613',\n",
       " 'mov dword ptr [rbp - 0x1c], 0',\n",
       " 'jmp 0x4629',\n",
       " 'mov eax, dword ptr [rbp - 0x18]',\n",
       " 'sub eax, dword ptr [rbp - 0x14]',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xd263]',\n",
       " 'cmp edx, eax',\n",
       " 'cmovle eax, edx',\n",
       " 'mov dword ptr [rbp - 0x1c], eax',\n",
       " 'mov eax, dword ptr [rip + 0xd331]',\n",
       " 'movsxd rdx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xd237]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3700',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'movsxd rbx, eax',\n",
       " 'mov rdx, qword ptr [rip + 0xd20d]',\n",
       " 'mov eax, dword ptr [rbp - 0x24]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 3',\n",
       " 'add rax, rdx',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov eax, dword ptr [rbp - 0x1c]',\n",
       " 'cdqe ',\n",
       " 'add rax, rdx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'cmp rbx, rax',\n",
       " 'ja 0x467e',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'cdqe ',\n",
       " 'jmp 0x46a4',\n",
       " 'mov rdx, qword ptr [rip + 0xd1db]',\n",
       " 'mov eax, dword ptr [rbp - 0x24]',\n",
       " 'cdqe ',\n",
       " 'shl rax, 3',\n",
       " 'add rax, rdx',\n",
       " 'mov rdx, qword ptr [rax]',\n",
       " 'mov eax, dword ptr [rbp - 0x1c]',\n",
       " 'cdqe ',\n",
       " 'add rax, rdx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3670',\n",
       " 'mov rcx, qword ptr [rip + 0xd1b5]',\n",
       " 'mov edx, dword ptr [rbp - 0x24]',\n",
       " 'movsxd rdx, edx',\n",
       " 'shl rdx, 3',\n",
       " 'add rdx, rcx',\n",
       " 'mov rcx, qword ptr [rdx]',\n",
       " 'mov edx, dword ptr [rbp - 0x1c]',\n",
       " 'movsxd rdx, edx',\n",
       " 'lea rsi, [rcx + rdx]',\n",
       " 'mov rdx, qword ptr [rip + 0xd1a4]',\n",
       " 'lea rcx, [rdx + 2]',\n",
       " 'mov rdx, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3820',\n",
       " 'mov eax, dword ptr [rip + 0xd19b]',\n",
       " 'cmp dword ptr [rbp - 0x24], eax',\n",
       " 'jne 0x46ed',\n",
       " 'mov edx, 0x3e',\n",
       " 'jmp 0x46f2',\n",
       " 'mov edx, 0x20',\n",
       " 'mov rax, qword ptr [rip + 0xd177]',\n",
       " 'mov byte ptr [rax], dl',\n",
       " 'mov rax, qword ptr [rip + 0xd16e]',\n",
       " 'add rax, 1',\n",
       " 'mov byte ptr [rax], 0x20',\n",
       " 'cmp dword ptr [rbp - 0x28], 0',\n",
       " 'je 0x475d',\n",
       " 'mov eax, dword ptr [rip + 0xb98b]',\n",
       " 'mov edi, eax',\n",
       " 'call 0x7547',\n",
       " 'mov eax, dword ptr [rip + 0xd15a]',\n",
       " 'cmp dword ptr [rbp - 0x24], eax',\n",
       " 'jne 0x4743',\n",
       " 'mov eax, dword ptr [rip + 0xb96f]',\n",
       " 'mov edi, eax',\n",
       " 'call 0x74f4',\n",
       " 'mov eax, dword ptr [rip + 0xb95e]',\n",
       " 'mov edi, eax',\n",
       " 'call 0x751c',\n",
       " 'jmp 0x475d',\n",
       " 'mov eax, dword ptr [rip + 0xb94f]',\n",
       " 'mov edi, eax',\n",
       " 'call 0x74f4',\n",
       " 'mov eax, dword ptr [rip + 0xb946]',\n",
       " 'mov edi, eax',\n",
       " 'call 0x751c',\n",
       " 'mov edx, dword ptr [rip + 0xd115]',\n",
       " 'mov eax, dword ptr [rbp - 0x24]',\n",
       " 'sub eax, edx',\n",
       " 'mov ecx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xd127]',\n",
       " 'mov edx, 0',\n",
       " 'mov esi, ecx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " 'mov edx, dword ptr [rip + 0xd1da]',\n",
       " 'mov rcx, qword ptr [rip + 0xd0e3]',\n",
       " 'mov rax, qword ptr [rip + 0xd104]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x912b',\n",
       " 'mov eax, dword ptr [rip + 0xd1bb]',\n",
       " 'lea edx, [rax - 1]',\n",
       " 'mov rax, qword ptr [rip + 0xd0f1]',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x922f',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our 50/50 NIP training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10221\n",
      "['endbr64 ', 'push rbp', 'mov rbp, rsp', 'mov rdx, qword ptr [rip + 0x2d98]', 'mov rax, qword ptr [rip + 0x2d81]']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "history = []\n",
    "next_instruction = []\n",
    "label = []\n",
    "\n",
    "page_len = 5\n",
    "instruction_pages = []\n",
    "for instruction_cluster in text:\n",
    "    instructions = [\n",
    "        instruction for instruction in instruction_cluster.split(delim) if instruction != ''\n",
    "    ]\n",
    "    if len(instructions)>page_len:\n",
    "        \n",
    "        for i in range(0,len(instructions),page_len):\n",
    "            instruction_pages.append(instructions[i:i+page_len])\n",
    "        \n",
    "print(len(instruction_pages))\n",
    "print(instruction_pages[0])\n",
    "\n",
    "for instruction_page in instruction_pages:\n",
    "    \n",
    "#     instructions = [\n",
    "#         instruction for instruction in instruction_page.split(';') if instruction != ''\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     num_instructions = len(instruction_page)\n",
    "    \n",
    "    \n",
    "\n",
    "#     start = random.randint(0, num_instructions-2)\n",
    "    # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "    if random.random() >= 0.5:\n",
    "        # this is IsNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(instruction_page[-1])\n",
    "        label.append(0)\n",
    "    else:\n",
    "        index = random.randint(0, bag_size-1)\n",
    "        # this is NotNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(bag[index])\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10221\n",
      "0\n",
      "-> endbr64 ;push rbp;mov rbp, rsp;mov rdx, qword ptr [rip + 0x2d98] \n",
      "\n",
      "#  mov rax, qword ptr [rip + 0x2d81] \n",
      "\n",
      "0\n",
      "-> lea rcx, [rip + 0xd62];mov rsi, rcx;mov rdi, rax;mov eax, 0 \n",
      "\n",
      "#  call 0x1120 \n",
      "\n",
      "0\n",
      "-> mov edi, 1 \n",
      "\n",
      "#  call 0x1170 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(label))\n",
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print('->',history[i] , '\\n')\n",
    "    print('# ',next_instruction[i] , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for tokenization, this time we truncate/pad each token to the same length of *512* tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(history, next_instruction, return_tensors='pt', max_length=128, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *token_type_ids* tensors have been built correctly (eg **1** indicating sentence B tokens) by checking the first instance of *token_type_ids*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.token_type_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **0** tokens following our sentence B tokens correspond to *PAD* tokens.\n",
    "\n",
    "Alongside this, we need to create a *labels* tensor too - which corresponds to the values contained within our `label` variable. Our *labels* tensor must be a *LongTensor*, and we will need to transpose the tensor so that it matches our other tensors' dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = torch.LongTensor([label]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputs` tensors are now ready, and we can begin building the model input pipeline for training. We first create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto setting up the training loop. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for a couple of epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nahid/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|| 511/511 [03:27<00:00,  2.46it/s, loss=0.729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5039138943248532 0.49852239145260285 0.5424189957952016 0.51954513148543 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.511002444987775 0.5014749262536873 0.16983016983016982 0.25373134328358204 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 1: 100%|| 511/511 [03:28<00:00,  2.45it/s, loss=0.665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.49853228962818 0.49266031419005923 0.47316349245609696 0.48271511481201107 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.511002444987775 1.0 0.000999000999000999 0.001996007984031936 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 2: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5117416829745597 0.5062792415661167 0.5085332673757111 0.5074037512339585 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n",
      "/home/nahid/anaconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5105134474327628 0.0 0.0 0.0 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 3: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5221379647749511 0.5163934426229508 0.5298046005441504 0.5230130631180564 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5183374083129584 0.5078895463510849 0.5144855144855145 0.511166253101737 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 4: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5267857142857143 0.5218812877263581 0.5132327479594361 0.5175208878912583 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5188264058679707 0.5043701799485861 0.98001998001998 0.6659877800407331 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 5: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5406066536203522 0.5353187300024612 0.5379668562948305 0.536639526276832 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5242053789731052 0.5110062893081762 0.6493506493506493 0.5719313682358117 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 6: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.549779843444227 0.5467217346411978 0.5238684145436557 0.535051155740811 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.517359413202934 0.5035714285714286 0.986013986013986 0.6666666666666666 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 7: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5519814090019569 0.5512129380053908 0.5058125154588177 0.5275377273313555 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5501222493887531 0.525328330206379 0.8391608391608392 0.6461538461538461 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 8: 100%|| 511/511 [03:34<00:00,  2.38it/s, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5694716242661448 0.5694186355189806 0.5305466237942122 0.5492957746478874 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5535452322738387 0.5492170022371364 0.4905094905094905 0.5182058047493403 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 9: 100%|| 511/511 [03:32<00:00,  2.40it/s, loss=0.601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5798679060665362 0.5772750381291306 0.5617116002968093 0.5693869875893194 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5745721271393643 0.6139130434782609 0.35264735264735264 0.44796954314720816 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 10: 100%|| 511/511 [03:32<00:00,  2.40it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.5905088062622309 0.5854438160806491 0.5889191194657433 0.5871763255240445 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5677261613691932 0.5665529010238908 0.4975024975024975 0.5297872340425531 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 11: 100%|| 511/511 [03:33<00:00,  2.39it/s, loss=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.6135029354207436 0.6213912565301072 0.5589908483799159 0.5885416666666666 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5770171149144254 0.5665362035225049 0.5784215784215784 0.5724172021749876 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 12: 100%|| 511/511 [03:35<00:00,  2.37it/s, loss=0.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.636986301369863 0.6385666408868265 0.612663863467722 0.6253471345619793 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5638141809290954 0.5374056280027454 0.7822177822177823 0.6371033360455655 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 13: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.6571673189823874 0.6509985387238189 0.6611427158050952 0.6560314148975336 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:20<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5691931540342299 0.5423728813559322 0.7672327672327672 0.6354985519238725 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 14: 100%|| 511/511 [03:27<00:00,  2.46it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.68456457925636 0.6812778603268945 0.680435320306703 0.6808563296621706 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:20<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5965770171149144 0.5781527531083481 0.6503496503496503 0.61212976022567 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 15: 100%|| 511/511 [03:27<00:00,  2.46it/s, loss=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.7021771037181996 0.7035443037974684 0.6873608706406134 0.6953584386338045 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:20<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5887530562347189 0.5613496932515337 0.7312687312687313 0.6351409978308026 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 16: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.7215019569471625 0.7216365461847389 0.7111056146425921 0.7163323782234956 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5990220048899756 0.5949632738719832 0.5664335664335665 0.5803480040941658 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 17: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.7386252446183953 0.734382685686178 0.7385604748948801 0.7364656554445679 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5770171149144254 0.5512820512820513 0.7302697302697303 0.6282767511817792 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 18: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.7605185909980431 0.7579806978470676 0.7576057383131338 0.7577931716971795 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5843520782396088 0.5655951346655083 0.6503496503496503 0.6050185873605948 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 19: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.7820450097847358 0.7889087656529516 0.7635419243136284 0.7760180995475112 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5941320293398533 0.5827686350435625 0.6013986013986014 0.591937069813176 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 20: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.7977005870841487 0.8021755628636479 0.7843185753153599 0.7931465732866433 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.589242053789731 0.588170865279299 0.5364635364635365 0.5611285266457681 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 21: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.81176614481409 0.8243523316062176 0.7870393272322532 0.8052638238643552 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5921760391198044 0.5708227311280747 0.6723276723276723 0.6174311926605505 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 22: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8169031311154599 0.8272493573264782 0.7959436062329953 0.8112945922097567 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.60880195599022 0.6192170818505338 0.5214785214785215 0.5661605206073753 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 23: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8236301369863014 0.8337182448036952 0.8036111798169676 0.8183879093198992 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5951100244498777 0.6082603254067585 0.4855144855144855 0.54 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 24: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8319471624266145 0.8483424693291569 0.8038585209003215 0.825501651003302 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5990220048899756 0.5991237677984665 0.5464535464535465 0.5715778474399164 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 25: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8418542074363993 0.8578865174388339 0.8152362107346031 0.8360177552314522 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.6034229828850856 0.6067415730337079 0.5394605394605395 0.5711263881544157 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 26: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8516389432485323 0.8652555498193082 0.829087311402424 0.8467853985095365 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.6156479217603912 0.6254375729288215 0.5354645354645354 0.5769644779332616 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 27: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.0419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8546966731898239 0.869340232858991 0.8310660400692556 0.8497723823975721 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.6097799511002445 0.6045314109165808 0.5864135864135864 0.5953346855983772 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 28: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8565313111545988 0.8655119714722364 0.8404650012367054 0.8528046178943405 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.6078239608801956 0.610678531701891 0.5484515484515484 0.5778947368421052 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 29: 100%|| 511/511 [03:29<00:00,  2.44it/s, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8698630136986302 0.8875878220140515 0.8436804353203067 0.8650773522698452 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:21<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5995110024449878 0.6022471910112359 0.5354645354645354 0.5668958223162347 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 30: 100%|| 511/511 [03:28<00:00,  2.45it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.8716976516634051 0.8801422041645506 0.8572841949047737 0.868562836737251 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|| 128/128 [00:22<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION:  0.5995110024449878 0.5821299638989169 0.6443556443556444 0.6116642958748223 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/511 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 31: 100%|| 511/511 [03:38<00:00,  2.34it/s, loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0.87279843444227 0.8850987432675045 0.8535740786544646 0.8690506169730547 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_194620/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "  2%|                                          | 3/128 [00:00<00:20,  6.01it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    \n",
    "    predictions_all, ground_truths_all = None, None\n",
    "    \n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    for N,batch in enumerate(train_loop):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        prediction = torch.argmax(outputs.logits, axis=-1)\n",
    "        prediction = prediction.detach().cpu().numpy().flatten()\n",
    "        ground_truth = labels.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        if N==0:\n",
    "            predictions_all = prediction\n",
    "            ground_truths_all = ground_truth\n",
    "        else:\n",
    "            predictions_all   = np.concatenate((predictions_all, prediction))\n",
    "            ground_truths_all = np.concatenate((ground_truths_all, ground_truth))\n",
    "            \n",
    "\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        train_loop.set_description(f'Epoch {epoch}')\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    accuracy = (accuracy_score(ground_truths_all,predictions_all))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ground_truths_all,predictions_all, average='binary')\n",
    "    print(\"Training: \", accuracy, precision, recall, f1, _)\n",
    "    \n",
    "    \n",
    "    ### EVAL Validation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        v_predictions_all, v_ground_truths_all = None, None\n",
    "        validation_loop = tqdm(validation_loader, leave=True)\n",
    "        for N,v_batch in enumerate(validation_loop):\n",
    "            v_input_ids = v_batch['input_ids'].to(device)\n",
    "            v_attention_mask = v_batch['attention_mask'].to(device)\n",
    "            v_token_type_ids = v_batch['token_type_ids'].to(device)\n",
    "            v_labels = v_batch['labels'].to(device)\n",
    "            # process\n",
    "            v_outputs = model(v_input_ids, attention_mask=v_attention_mask,\n",
    "                            token_type_ids=v_token_type_ids,\n",
    "                            labels=v_labels)\n",
    "            v_prediction = torch.argmax(v_outputs.logits, axis=-1)\n",
    "            v_prediction = v_prediction.detach().cpu().numpy().flatten()\n",
    "            v_ground_truth = v_labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "            if N==0:\n",
    "                v_predictions_all = v_prediction\n",
    "                v_ground_truths_all = v_ground_truth\n",
    "            else:\n",
    "                v_predictions_all   = np.concatenate((v_predictions_all, v_prediction))\n",
    "                v_ground_truths_all = np.concatenate((v_ground_truths_all, v_ground_truth))\n",
    "\n",
    "        v_accuracy = (accuracy_score(v_ground_truths_all, v_predictions_all))\n",
    "        v_precision, v_recall, v_f1, _ = precision_recall_fscore_support(v_ground_truths_all, \n",
    "                                                                         v_predictions_all, average='binary')\n",
    "        print(\"VALIDATION: \",v_accuracy, v_precision, v_recall, v_f1, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model weights\n",
    "# training_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
