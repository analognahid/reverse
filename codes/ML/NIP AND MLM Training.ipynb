{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Instruction Prediction Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raisul/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gitwipe 4\n",
      "gitkeys 4\n",
      "gitview 140\n",
      "gitps 147\n",
      "gitfm 341\n",
      "gitwhich 6\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys,os\n",
    "from elftools.elf.elffile import ELFFile\n",
    "from elftools.elf.segments import Segment\n",
    "from capstone import *\n",
    "from capstone.x86 import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dir_path = \"./data/binaries/\"\n",
    "dir_file_list = os.listdir(data_dir_path)\n",
    "\n",
    "with open('./data/instruction_clusters.txt', 'w') as data_file:\n",
    "    for filename in dir_file_list:\n",
    "        filePath = os.path.join(data_dir_path,filename)\n",
    "\n",
    "        fh = open(filePath, 'rb')\n",
    "        bin_bytearray = bytearray(fh.read())\n",
    "        \n",
    "        with open(filePath, 'rb') as f:\n",
    "            elf = ELFFile(f)\n",
    "            dwarfinfo = elf.get_dwarf_info()\n",
    "            aranges = dwarfinfo.get_aranges()\n",
    "            print(filename, len(aranges.entries))\n",
    "            for arange in aranges.entries:\n",
    "\n",
    "                entry = arange.begin_addr\n",
    "                exit  = arange.begin_addr + arange.length\n",
    "                ops = bin_bytearray[entry: exit]\n",
    "\n",
    "                md = Cs(CS_ARCH_X86, CS_MODE_64)\n",
    "                md.detail = True\n",
    "                for inst in md.disasm(ops, entry):\n",
    "\n",
    "                    data_file.write(inst.mnemonic+\" \"+inst.op_str+\";\")\n",
    "                data_file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./binary-tokenizer\")\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim = ';'\n",
    "with open('./data/instruction_clusters.txt', 'r') as fp:\n",
    "    text = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split sentences into consecutive, and non-consecutive sequences.\n",
    "\n",
    "We have to deal with edge-cases too - for example where there is only a single sentence within a paragraph as with the three examples above (in comparison to below where we can easily split into multiple sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text[51].split(delim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assign a 50% probability of using the genuine next sentence, and 50% probability of using another random sentence.\n",
    "\n",
    "To make this simpler, we'll create a *'bag'* of individual sentences to pull from when selecting a random sentence B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n"
     ]
    }
   ],
   "source": [
    "bag = [instruction for instruction_cluster in text for instruction in instruction_cluster.split(delim)  if instruction!= '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'mov rdx, qword ptr [rip + 0x2d98]',\n",
       " 'mov rax, qword ptr [rip + 0x2d81]',\n",
       " 'lea rcx, [rip + 0xd62]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov edi, 1',\n",
       " 'call 0x1170',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x20',\n",
       " 'mov dword ptr [rbp - 0x14], edi',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'mov edx, 1',\n",
       " 'mov esi, 0',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1180',\n",
       " 'mov qword ptr [rbp - 0x10], rax',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'mov edx, 2',\n",
       " 'mov esi, 0',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1180',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov rcx, qword ptr [rbp - 0x10]',\n",
       " 'mov eax, dword ptr [rbp - 0x14]',\n",
       " 'mov edx, 0',\n",
       " 'mov rsi, rcx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1180',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x40',\n",
       " 'mov qword ptr [rbp - 0x38], rdi',\n",
       " 'mov rax, qword ptr [rbp - 0x38]',\n",
       " 'mov esi, 2',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1160',\n",
       " 'mov dword ptr [rbp - 0x2c], eax',\n",
       " 'cmp dword ptr [rbp - 0x2c], -1',\n",
       " 'jne 0x137a',\n",
       " 'mov rdx, qword ptr [rip + 0x2cdf]',\n",
       " 'mov rax, qword ptr [rip + 0x2cc8]',\n",
       " 'mov rcx, qword ptr [rbp - 0x38]',\n",
       " 'lea rsi, [rip + 0xcbd]',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov eax, 1',\n",
       " 'jmp 0x14cd',\n",
       " 'mov eax, dword ptr [rbp - 0x2c]',\n",
       " 'mov edi, eax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x12c0',\n",
       " 'mov qword ptr [rbp - 0x18], rax',\n",
       " 'cmp qword ptr [rbp - 0x18], 0',\n",
       " 'jne 0x139e',\n",
       " 'mov eax, 0',\n",
       " 'jmp 0x14cd',\n",
       " 'mov edi, 0x10000',\n",
       " 'call 0x1150',\n",
       " 'mov qword ptr [rbp - 0x10], rax',\n",
       " 'cmp qword ptr [rbp - 0x10], 0',\n",
       " 'jne 0x13e2',\n",
       " 'mov rdx, qword ptr [rip + 0x2c76]',\n",
       " 'mov rax, qword ptr [rip + 0x2c5f]',\n",
       " 'lea rcx, [rip + 0xc78]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov eax, 1',\n",
       " 'jmp 0x14cd',\n",
       " 'mov qword ptr [rbp - 0x28], 0',\n",
       " 'jmp 0x14ab',\n",
       " 'mov rax, qword ptr [rbp - 0x18]',\n",
       " 'sub rax, qword ptr [rbp - 0x28]',\n",
       " 'mov edx, 0x10000',\n",
       " 'cmp rax, rdx',\n",
       " 'cmovg rax, rdx',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov qword ptr [rbp - 0x20], 0',\n",
       " 'jmp 0x1451',\n",
       " 'call 0x1190',\n",
       " 'movsxd rdx, eax',\n",
       " 'imul rdx, rdx, -0x7f7f7f7f',\n",
       " 'shr rdx, 0x20',\n",
       " 'add edx, eax',\n",
       " 'sar edx, 7',\n",
       " 'mov esi, eax',\n",
       " 'sar esi, 0x1f',\n",
       " 'mov ecx, edx',\n",
       " 'sub ecx, esi',\n",
       " 'mov edx, ecx',\n",
       " 'shl edx, 8',\n",
       " 'sub edx, ecx',\n",
       " 'sub eax, edx',\n",
       " 'mov ecx, eax',\n",
       " 'mov rdx, qword ptr [rbp - 0x20]',\n",
       " 'mov rax, qword ptr [rbp - 0x10]',\n",
       " 'add rax, rdx',\n",
       " 'mov edx, ecx',\n",
       " 'mov byte ptr [rax], dl',\n",
       " 'add qword ptr [rbp - 0x20], 1',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'cmp rax, qword ptr [rbp - 8]',\n",
       " 'jl 0x1411',\n",
       " 'mov rdx, qword ptr [rbp - 8]',\n",
       " 'mov rcx, qword ptr [rbp - 0x10]',\n",
       " 'mov eax, dword ptr [rbp - 0x2c]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x10f0',\n",
       " 'cmp qword ptr [rbp - 8], rax',\n",
       " 'je 0x14a3',\n",
       " 'mov rdx, qword ptr [rip + 0x2bb3]',\n",
       " 'mov rax, qword ptr [rip + 0x2b9c]',\n",
       " 'mov rcx, qword ptr [rbp - 0x38]',\n",
       " 'lea rsi, [rip + 0xbd0]',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov eax, 1',\n",
       " 'jmp 0x14cd',\n",
       " 'add qword ptr [rbp - 0x28], 0x10000',\n",
       " 'mov rax, qword ptr [rbp - 0x28]',\n",
       " 'cmp rax, qword ptr [rbp - 0x18]',\n",
       " 'jl 0x13ef',\n",
       " 'mov eax, dword ptr [rbp - 0x2c]',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1100',\n",
       " 'call 0x1140',\n",
       " 'mov eax, 0',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x20',\n",
       " 'mov dword ptr [rbp - 0x14], edi',\n",
       " 'mov qword ptr [rbp - 0x20], rsi',\n",
       " 'mov dword ptr [rbp - 4], 0',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'mov rax, qword ptr [rax]',\n",
       " 'mov qword ptr [rip + 0x2b39], rax',\n",
       " 'cmp dword ptr [rbp - 0x14], 1',\n",
       " 'jg 0x1507',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1289',\n",
       " 'mov edi, 0',\n",
       " 'call 0x1130',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1110',\n",
       " 'mov dword ptr [rbp - 8], 1',\n",
       " 'jmp 0x154c',\n",
       " 'mov eax, dword ptr [rbp - 8]',\n",
       " 'cdqe ',\n",
       " 'lea rdx, [rax*8]',\n",
       " 'mov rax, qword ptr [rbp - 0x20]',\n",
       " 'add rax, rdx',\n",
       " 'mov rax, qword ptr [rax]',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x131b',\n",
       " 'add dword ptr [rbp - 4], eax',\n",
       " 'add dword ptr [rbp - 8], 1',\n",
       " 'mov eax, dword ptr [rbp - 8]',\n",
       " 'cmp eax, dword ptr [rbp - 0x14]',\n",
       " 'jl 0x1521',\n",
       " 'cmp dword ptr [rbp - 4], 0',\n",
       " 'setne al',\n",
       " 'movzx eax, al',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'lea rax, [rip + 0x2d88]',\n",
       " 'mov rsi, rax',\n",
       " 'mov edi, 1',\n",
       " 'call 0x1180',\n",
       " 'mov rax, qword ptr [rip + 0x2d74]',\n",
       " 'mov rdx, qword ptr [rip + 0x2d75]',\n",
       " 'mov qword ptr [rip + 0x2da6], rax',\n",
       " 'mov qword ptr [rip + 0x2da7], rdx',\n",
       " 'mov rax, qword ptr [rip + 0x2d68]',\n",
       " 'mov rdx, qword ptr [rip + 0x2d69]',\n",
       " 'mov qword ptr [rip + 0x2d9a], rax',\n",
       " 'mov qword ptr [rip + 0x2d9b], rdx',\n",
       " 'mov rax, qword ptr [rip + 0x2d5c]',\n",
       " 'mov rdx, qword ptr [rip + 0x2d5d]',\n",
       " 'mov qword ptr [rip + 0x2d8e], rax',\n",
       " 'mov qword ptr [rip + 0x2d8f], rdx',\n",
       " 'mov rax, qword ptr [rip + 0x2d50]',\n",
       " 'mov qword ptr [rip + 0x2d89], rax',\n",
       " 'mov eax, dword ptr [rip + 0x2d4b]',\n",
       " 'mov dword ptr [rip + 0x2d85], eax',\n",
       " 'mov eax, dword ptr [rip + 0x2d47]',\n",
       " 'and eax, 0xfffffa3c',\n",
       " 'mov dword ptr [rip + 0x2d3c], eax',\n",
       " 'mov eax, dword ptr [rip + 0x2d3a]',\n",
       " 'and eax, 0xfffffffe',\n",
       " 'mov dword ptr [rip + 0x2d31], eax',\n",
       " 'mov eax, dword ptr [rip + 0x2d33]',\n",
       " 'or al, 0x81',\n",
       " 'mov dword ptr [rip + 0x2d2b], eax',\n",
       " 'mov eax, dword ptr [rip + 0x2d25]',\n",
       " 'and eax, 0xfffffff5',\n",
       " 'mov dword ptr [rip + 0x2d1c], eax',\n",
       " 'mov byte ptr [rip + 0x2d1a], 0',\n",
       " 'mov byte ptr [rip + 0x2d14], 0',\n",
       " 'mov byte ptr [rip + 0x2d14], 0',\n",
       " 'mov byte ptr [rip + 0x2d0e], 0',\n",
       " 'mov byte ptr [rip + 0x2d04], 1',\n",
       " 'mov byte ptr [rip + 0x2cfc], 0',\n",
       " 'mov byte ptr [rip + 0x2cf2], 0',\n",
       " 'mov byte ptr [rip + 0x2cec], 0',\n",
       " 'mov byte ptr [rip + 0x2ced], 0',\n",
       " 'mov byte ptr [rip + 0x2ceb], 0',\n",
       " 'mov byte ptr [rip + 0x2cde], 0',\n",
       " 'mov byte ptr [rip + 0x2cd9], 0',\n",
       " 'mov byte ptr [rip + 0x2cd3], 0',\n",
       " 'mov byte ptr [rip + 0x2ccd], 0',\n",
       " 'mov byte ptr [rip + 0x2cc7], 0',\n",
       " 'lea rax, [rip + 0x2ca0]',\n",
       " 'mov rdx, rax',\n",
       " 'mov esi, 1',\n",
       " 'mov edi, 1',\n",
       " 'call 0x1190',\n",
       " 'lea rax, [rip + 0x2c87]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x1170',\n",
       " 'mov dword ptr [rip + 0x2c25], eax',\n",
       " 'mov edx, 1',\n",
       " 'mov esi, 0x540a',\n",
       " 'mov edi, 1',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1130',\n",
       " 'mov esi, 1',\n",
       " 'mov edi, 1',\n",
       " 'call 0x11a0',\n",
       " 'nop ',\n",
       " 'pop rbp',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'lea rax, [rip + 0x2bff]',\n",
       " 'mov rdx, rax',\n",
       " 'mov esi, 1',\n",
       " 'mov edi, 1',\n",
       " 'call 0x1190',\n",
       " 'nop ',\n",
       " 'pop rbp',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x10',\n",
       " 'mov dword ptr [rbp - 4], edi',\n",
       " 'mov eax, dword ptr [rbp - 4]',\n",
       " 'lea rdx, [rip - 0x19]',\n",
       " 'mov rsi, rdx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x1150',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1432',\n",
       " 'mov edi, 1',\n",
       " 'call 0x11b0',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x10',\n",
       " 'mov rax, qword ptr fs:[0x28]',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'xor eax, eax',\n",
       " 'mov rax, qword ptr [rip + 0x2b61]',\n",
       " 'lea rdx, [rip + 0xce4]',\n",
       " 'lea rcx, [rip + 0xce3]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'lea rax, [rip - 0x7e]',\n",
       " 'mov rsi, rax',\n",
       " 'mov edi, 0xf',\n",
       " 'call 0x1150',\n",
       " 'mov eax, 0',\n",
       " 'call 0x12a9',\n",
       " 'mov esi, 1',\n",
       " 'mov edi, 0x14',\n",
       " 'call 0x1150',\n",
       " 'mov esi, 1',\n",
       " 'mov edi, 0x12',\n",
       " 'call 0x1150',\n",
       " 'lea rax, [rbp - 9]',\n",
       " 'mov edx, 1',\n",
       " 'mov rsi, rax',\n",
       " 'mov edi, 0',\n",
       " 'call 0x1140',\n",
       " 'movzx eax, byte ptr [rbp - 9]',\n",
       " 'cmp al, 0x20',\n",
       " 'je 0x1555',\n",
       " 'movzx eax, byte ptr [rbp - 9]',\n",
       " 'movzx eax, al',\n",
       " 'mov esi, eax',\n",
       " 'lea rax, [rip + 0xc7f]',\n",
       " 'mov rdi, rax',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1120',\n",
       " 'mov rax, qword ptr [rip + 0x2ad5]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x1160',\n",
       " 'jmp 0x1509',\n",
       " 'nop ',\n",
       " 'mov eax, 0',\n",
       " 'call 0x1432',\n",
       " 'mov edi, 0xa',\n",
       " 'call 0x1100',\n",
       " 'mov eax, 0',\n",
       " 'mov rdx, qword ptr [rbp - 8]',\n",
       " 'sub rdx, qword ptr fs:[0x28]',\n",
       " 'je 0x1583',\n",
       " 'call 0x1110',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'sub rsp, 0x10',\n",
       " 'mov eax, dword ptr [rip + 0xbfd1]',\n",
       " 'test eax, eax',\n",
       " 'jne 0x3a26',\n",
       " 'mov eax, 0x7fffffff',\n",
       " 'jmp 0x3a79',\n",
       " 'mov eax, dword ptr [rip + 0xbfbc]',\n",
       " 'mov edx, 1',\n",
       " 'mov esi, 0',\n",
       " 'mov edi, eax',\n",
       " 'call 0x38d0',\n",
       " 'mov qword ptr [rbp - 0x10], rax',\n",
       " 'mov eax, dword ptr [rip + 0xbfa1]',\n",
       " 'mov edx, 2',\n",
       " 'mov esi, 0',\n",
       " 'mov edi, eax',\n",
       " 'call 0x38d0',\n",
       " 'mov qword ptr [rbp - 8], rax',\n",
       " 'mov eax, dword ptr [rip + 0xbf86]',\n",
       " 'mov rcx, qword ptr [rbp - 0x10]',\n",
       " 'mov edx, 0',\n",
       " 'mov rsi, rcx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x38d0',\n",
       " 'mov rax, qword ptr [rbp - 8]',\n",
       " 'leave ',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'mov eax, dword ptr [rip + 0xc033]',\n",
       " 'cmp eax, 8',\n",
       " 'jle 0x3ac5',\n",
       " 'lea rax, [rip + 0xb803]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x35d0',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xbf0b]',\n",
       " 'add eax, edx',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xc00d]',\n",
       " 'lea ecx, [rax - 5]',\n",
       " 'mov rax, qword ptr [rip + 0xbf57]',\n",
       " 'mov esi, ecx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x8794',\n",
       " 'jmp 0x3ae8',\n",
       " 'mov eax, dword ptr [rip + 0xbff5]',\n",
       " 'lea edx, [rax - 1]',\n",
       " 'mov eax, dword ptr [rip + 0xbfe8]',\n",
       " 'lea ecx, [rax - 1]',\n",
       " 'mov rax, qword ptr [rip + 0xbf32]',\n",
       " 'mov esi, ecx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x8794',\n",
       " 'nop ',\n",
       " 'pop rbp',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'mov eax, dword ptr [rip + 0xbfc7]',\n",
       " 'movsxd rdx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xbedd]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3620',\n",
       " 'mov rax, qword ptr [rip + 0xbf09]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x35d0',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xbf99]',\n",
       " 'cmp edx, eax',\n",
       " 'jge 0x3b3e',\n",
       " 'mov rax, qword ptr [rip + 0xbeee]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x35d0',\n",
       " 'cdqe ',\n",
       " 'jmp 0x3b46',\n",
       " 'mov eax, dword ptr [rip + 0xbf7c]',\n",
       " 'cdqe ',\n",
       " 'mov rsi, qword ptr [rip + 0xbed3]',\n",
       " 'mov rcx, qword ptr [rip + 0xbe8c]',\n",
       " 'mov rdx, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3730',\n",
       " 'mov edx, dword ptr [rip + 0xb5ff]',\n",
       " 'mov ecx, dword ptr [rip + 0xb5f5]',\n",
       " 'mov eax, dword ptr [rip + 0xb5f7]',\n",
       " 'mov esi, ecx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x6aff',\n",
       " 'mov rax, qword ptr [rip + 0xbe7f]',\n",
       " 'mov edx, 0',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x8794',\n",
       " 'mov edx, dword ptr [rip + 0xbf27]',\n",
       " 'mov rcx, qword ptr [rip + 0xbe40]',\n",
       " 'mov rax, qword ptr [rip + 0xbe59]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x8690',\n",
       " 'nop ',\n",
       " 'pop rbp',\n",
       " 'ret ',\n",
       " 'endbr64 ',\n",
       " 'push rbp',\n",
       " 'mov rbp, rsp',\n",
       " 'mov eax, dword ptr [rip + 0xbefd]',\n",
       " 'movsxd rdx, eax',\n",
       " 'mov rax, qword ptr [rip + 0xbe13]',\n",
       " 'mov esi, 0x20',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x3620',\n",
       " 'mov rax, qword ptr [rip + 0xbdef]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x35d0',\n",
       " 'mov edx, eax',\n",
       " 'mov eax, dword ptr [rip + 0xbecf]',\n",
       " 'cmp edx, eax',\n",
       " 'jge 0x3c08',\n",
       " 'mov rax, qword ptr [rip + 0xbdd4]',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x35d0',\n",
       " 'cdqe ',\n",
       " 'jmp 0x3c10',\n",
       " 'mov eax, dword ptr [rip + 0xbeb2]',\n",
       " 'cdqe ',\n",
       " 'mov rsi, qword ptr [rip + 0xbdb9]',\n",
       " 'mov rcx, qword ptr [rip + 0xbdc2]',\n",
       " 'mov rdx, rax',\n",
       " 'mov rdi, rcx',\n",
       " 'call 0x3730',\n",
       " 'mov edx, dword ptr [rip + 0xb541]',\n",
       " 'mov ecx, dword ptr [rip + 0xb537]',\n",
       " 'mov eax, dword ptr [rip + 0xb539]',\n",
       " 'mov esi, ecx',\n",
       " 'mov edi, eax',\n",
       " 'call 0x6aff',\n",
       " 'mov rax, qword ptr [rip + 0xbdbd]',\n",
       " 'mov edx, 0',\n",
       " 'mov esi, 0',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x8794',\n",
       " 'mov edx, dword ptr [rip + 0xbe5d]',\n",
       " 'mov rcx, qword ptr [rip + 0xbd76]',\n",
       " 'mov rax, qword ptr [rip + 0xbd97]',\n",
       " 'mov rsi, rcx',\n",
       " 'mov rdi, rax',\n",
       " 'call 0x8690',\n",
       " 'nop ',\n",
       " 'pop rbp',\n",
       " 'ret ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our 50/50 NIP training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "['endbr64 ', 'push rbp', 'mov rbp, rsp', 'mov rdx, qword ptr [rip + 0x2d98]', 'mov rax, qword ptr [rip + 0x2d81]']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "history = []\n",
    "next_instruction = []\n",
    "label = []\n",
    "\n",
    "page_len = 5\n",
    "instruction_pages = []\n",
    "for instruction_cluster in text:\n",
    "    instructions = [\n",
    "        instruction for instruction in instruction_cluster.split(delim) if instruction != ''\n",
    "    ]\n",
    "    if len(instructions)>page_len:\n",
    "        \n",
    "        for i in range(0,len(instructions),page_len):\n",
    "            instruction_pages.append(instructions[i:i+page_len])\n",
    "        \n",
    "print(len(instruction_pages))\n",
    "print(instruction_pages[0])\n",
    "\n",
    "for instruction_page in instruction_pages:\n",
    "    \n",
    "#     instructions = [\n",
    "#         instruction for instruction in instruction_page.split(';') if instruction != ''\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     num_instructions = len(instruction_page)\n",
    "    \n",
    "    \n",
    "\n",
    "#     start = random.randint(0, num_instructions-2)\n",
    "    # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "    if random.random() >= 0.5:\n",
    "        # this is IsNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(instruction_page[-1])\n",
    "        label.append(0)\n",
    "    else:\n",
    "        index = random.randint(0, bag_size-1)\n",
    "        # this is NotNextSentence\n",
    "        history.append(delim.join(instruction_page[:-1]))\n",
    "        next_instruction.append(bag[index])\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "1\n",
      "-> endbr64 ;push rbp;mov rbp, rsp;mov rdx, qword ptr [rip + 0x2d98] \n",
      "\n",
      "#  movzx eax, al \n",
      "\n",
      "0\n",
      "-> lea rcx, [rip + 0xd62];mov rsi, rcx;mov rdi, rax;mov eax, 0 \n",
      "\n",
      "#  call 0x1120 \n",
      "\n",
      "1\n",
      "-> mov edi, 1 \n",
      "\n",
      "#  nop  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(label))\n",
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print('->',history[i] , '\\n')\n",
    "    print('# ',next_instruction[i] , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for tokenization, this time we truncate/pad each token to the same length of *512* tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(history, next_instruction, return_tensors='pt', \n",
    "                   max_length=128, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *token_type_ids* tensors have been built correctly (eg **1** indicating sentence B tokens) by checking the first instance of *token_type_ids*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.token_type_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **0** tokens following our sentence B tokens correspond to *PAD* tokens.\n",
    "\n",
    "Alongside this, we need to create a *labels* tensor too - which corresponds to the values contained within our `label` variable. Our *labels* tensor must be a *LongTensor*, and we will need to transpose the tensor so that it matches our other tensors' dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the labels tensor is simply a clone of the input_ids tensor before masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2, 180,   1,  ...,   0,   0,   0],\n",
       "        [  2, 116, 117,  ...,   0,   0,   0],\n",
       "        [  2,  86, 164,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  2,  86, 123,  ...,   0,   0,   0],\n",
       "        [  2,   3, 183,  ...,   0,   0,   0],\n",
       "        [  2, 180,   1,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we mask tokens in the input_ids tensor using the 15% probability for MLM - ensuring we don't mask CLS, SEP, or PAD tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr[0]\n",
    "# inputs.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now take the indices of each True value within each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 27]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then apply these indices to each row in input_ids, assigning each value at these indices a value of 103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputs` tensors are now ready, and we can begin building the model input pipeline for training. We first create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_loader      = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto setting up the training loop. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for a couple of epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]/tmp/ipykernel_3078/81523791.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "tensor([[ 0.8613, -0.1931],\n",
      "        [ 0.5191,  0.0642],\n",
      "        [ 0.9090, -0.1997],\n",
      "        [ 0.8580, -0.1402],\n",
      "        [ 0.4152,  0.2810],\n",
      "        [ 0.6410,  0.1175],\n",
      "        [ 0.8547, -0.1468],\n",
      "        [ 0.7451,  0.0344]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "NextSentencePredictorOutput(loss=tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8613, -0.1931],\n",
      "        [ 0.5191,  0.0642],\n",
      "        [ 0.9090, -0.1997],\n",
      "        [ 0.8580, -0.1402],\n",
      "        [ 0.4152,  0.2810],\n",
      "        [ 0.6410,  0.1175],\n",
      "        [ 0.8547, -0.1468],\n",
      "        [ 0.7451,  0.0344]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    \n",
    "    predictions_all, ground_truths_all = None, None\n",
    "    \n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    for N,batch in enumerate(train_loop):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        next_sentence_label = batch['next_sentence_label'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        next_sentence_label=next_sentence_label,\n",
    "                        labels=labels)\n",
    "        print((outputs.logits.shape))\n",
    "        print(outputs.logits)\n",
    "        print(outputs)\n",
    "        prediction = torch.argmax(outputs.logits, axis=-1)\n",
    "        print(prediction)\n",
    "        break\n",
    "#         prediction = prediction.detach().cpu().numpy().flatten()\n",
    "#         ground_truth = labels.detach().cpu().numpy().flatten()\n",
    "        \n",
    "#         if N==0:\n",
    "#             predictions_all = prediction\n",
    "#             ground_truths_all = ground_truth\n",
    "#         else:\n",
    "#             predictions_all   = np.concatenate((predictions_all, prediction))\n",
    "#             ground_truths_all = np.concatenate((ground_truths_all, ground_truth))\n",
    "            \n",
    "\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        train_loop.set_description(f'Epoch {epoch}')\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "#     accuracy = (accuracy_score(ground_truths_all,predictions_all))\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(ground_truths_all,predictions_all, average='binary')\n",
    "#     print(\"Training: \", accuracy, precision, recall, f1, _)\n",
    "    \n",
    "    \n",
    "    ### EVAL Validation\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         v_predictions_all, v_ground_truths_all = None, None\n",
    "#         validation_loop = tqdm(validation_loader, leave=True)\n",
    "#         for N,v_batch in enumerate(validation_loop):\n",
    "#             v_input_ids = v_batch['input_ids'].to(device)\n",
    "#             v_attention_mask = v_batch['attention_mask'].to(device)\n",
    "#             v_token_type_ids = v_batch['token_type_ids'].to(device)\n",
    "#             v_labels = v_batch['labels'].to(device)\n",
    "#             # process\n",
    "#             v_outputs = model(v_input_ids, attention_mask=v_attention_mask,\n",
    "#                             token_type_ids=v_token_type_ids,\n",
    "#                             labels=v_labels)\n",
    "#             v_prediction = torch.argmax(v_outputs.logits, axis=-1)\n",
    "#             v_prediction = v_prediction.detach().cpu().numpy().flatten()\n",
    "#             v_ground_truth = v_labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "#             if N==0:\n",
    "#                 v_predictions_all = v_prediction\n",
    "#                 v_ground_truths_all = v_ground_truth\n",
    "#             else:\n",
    "#                 v_predictions_all   = np.concatenate((v_predictions_all, v_prediction))\n",
    "#                 v_ground_truths_all = np.concatenate((v_ground_truths_all, v_ground_truth))\n",
    "\n",
    "#         v_accuracy = (accuracy_score(v_ground_truths_all, v_predictions_all))\n",
    "#         v_precision, v_recall, v_f1, _ = precision_recall_fscore_support(v_ground_truths_all, \n",
    "#                                                                          v_predictions_all, average='binary')\n",
    "#         print(\"VALIDATION: \",v_accuracy, v_precision, v_recall, v_f1, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model weights\n",
    "# training_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
